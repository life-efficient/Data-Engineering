{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159173ce",
   "metadata": {},
   "source": [
    "# Spark basics\n",
    "\n",
    "As most of the course is `python` centered we will use `python` frontend (a.k.a. `pyspark`) to interact with the cluster.\n",
    "\n",
    "# Installation\n",
    "\n",
    "PySpark has many dependencies, not only with other Python packages, but also with other modules that are not easily installed using the convenient `pip install` command. While you can install pyspark using `pip install pyspark` this is probably not going to be enough. Therefore, we recommend you to follow the next steps:\n",
    "\n",
    "1. Visit [PySpark download page](https://spark.apache.org/downloads.html) and:\n",
    "- Choose latest release\n",
    "- Download package locally\n",
    "\n",
    "2. Create a folder (for example `spark`)  in a directory that you know will be safe. `~/` is usually a good option. \n",
    "3. Extract the files from the downloaded file into the created folder. At the time of writing, the last version was Spark 3.1.2, so, in that case, your directory will look like this (in case you are using the same examples):\n",
    "```\n",
    "~/\n",
    "│\n",
    "├── spark/\n",
    "│   └── spark-3.1.2-bin-hadoop3.2  <--- SPARK_HOME\n",
    "│         ├── bin\n",
    "│         ├── conf\n",
    "│         ├── data\n",
    "... \n",
    "```\n",
    "4. It is important you set the directory as SPARK_HOME, otherwise, PySpark won't know where to find the corresponding commands. To do so, simply set it as a environment variable copying the following command in your `~/.bashrc` file:\n",
    "\n",
    "`export SPARK_HOME=<path to your home directory>/spark/spark-3.1.2-bin-hadoop3.2`\n",
    "\n",
    "<font size=-1.5><i> The command above depends on where you extracted the files you downloaded and the version</i></font>\n",
    "\n",
    "5. Save your `~/.bashrc` and restart your computer. You should be able to use PySpark now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108de564",
   "metadata": {},
   "source": [
    "6. To check if the installation was successful, you can install findspark (`pip install findspark`) and run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0262f36c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14164/1796740182.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332c901",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "  <summary> <font size=+1> For Windows Users </font> </summary>\n",
    "  \n",
    "  Depending on your environment, the last steps might not work. In that case, you have to set the environment variable manually. Look at the following gif to know how to it\n",
    "\n",
    "  <p align=center><img src=images/Spark_home.gif></p>\n",
    "\n",
    "  If this still doesn't work\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114af81",
   "metadata": {},
   "source": [
    "## findspark\n",
    "\n",
    "The Spark functionalities might not be discoverable within a script or a notebook, so you can use `findspark` which will set the script or notebook to keep using Spark interactively. Remember that:\n",
    "\n",
    "1. Inside the script you are going to define the instructions\n",
    "2. Those instructions will be orchestrated amongst the executors using Spark\n",
    "3. PySpark will be the API that helps you write in Python the instructions. Then, those instructions will be translated, so Spark actually understands it\n",
    "\n",
    "Thus, you will create the script using PySpark, and then, you will send that script to Spark, usually using spark-submit, which we will see later in this notebook.\n",
    "\n",
    "`findspark` will be useful when you are developing your application, to check if spark will respond the way you expect while you are writing your code.\n",
    "\n",
    "- Run `findspark.init()` (which will set up necessary environment variables so `pyspark` can connect to JVM)\n",
    "- You can also tun `findspark.find()` to see the directory where `SPARK_HOME` has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e915bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ca5fa",
   "metadata": {},
   "source": [
    "## Spark config\n",
    "\n",
    "Given all of the steps above, we can set up `spark` distributed engine via:\n",
    "- Frontend (`pyspark` in our case) - usable for application specific tasks and varying configuration\n",
    "- Command line - usable for `spark-submit` and __overriding default values__\n",
    "- Config file - usable as a base config and __when we submit job to the cluster__\n",
    "- Global config file\n",
    "\n",
    "> Above is also a priority list and the config at certain position ovverides values from the ones below it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254d4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1g\n",
      "spark.master=local[8]\n",
      "spark.app.name=TestApp\n",
      "spark.eventLog.enabled=False\n",
      "spark.executorEnv.VAR3=value3\n",
      "spark.executorEnv.VAR4=value4\n",
      "spark.executor.memory=1g\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import pyspark\n",
    "\n",
    "cfg = (\n",
    "    pyspark.SparkConf()\n",
    "    # Setting where master node is located [cores for multiprocessing]\n",
    "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
    "    # Setting application name\n",
    "    .setAppName(\"TestApp\")\n",
    "    # Setting config value via string\n",
    "    .set(\"spark.eventLog.enabled\", False)\n",
    "    # Setting environment variables for executors to use\n",
    "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
    "    # Setting memory if this setting was not set previously\n",
    "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
    ")\n",
    "\n",
    "# Getting a single variable\n",
    "print(cfg.get(\"spark.executor.memory\"))\n",
    "# Listing all of them in string readable format\n",
    "print(cfg.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f8165",
   "metadata": {},
   "source": [
    "# Sessions\n",
    "\n",
    "> PySpark provides session object __which represents UNIFIED connection to Spark cluster__.\n",
    "\n",
    "There are a few ways to set it up:\n",
    "- directly through named/unnamed arguments\n",
    "- __using `SparkConf` object__ (which we created and will use)\n",
    "- Providing `SparkContext` with settings (__deprecated, avoid__)\n",
    "\n",
    "It is used to:\n",
    "- create `DataFrame`s (__main object containing data within cluster__)\n",
    "- __broadcast variables to machines within the cluster__\n",
    "- Run operations across HDFS enabled systems\n",
    "\n",
    "There are some confusing points though...\n",
    "\n",
    "## Contexts, Sessions and all of that...\n",
    "\n",
    "`Spark` and `pyspark` provide a few objects to interact with `Spark` engine:\n",
    "- `pyspark.SparkContext`\n",
    "- __Only `scala`__: `org.apache.spark.sql.SQLContext`\n",
    "- __Only `scala`__: `org.apache.spark.sql.hive.HiveContext`\n",
    "- `pyspark.sql.SparkContext`\n",
    "\n",
    "So what's the deal?\n",
    "\n",
    "### SparkContext\n",
    "\n",
    "> Object used by driver to communicate with cluster manager, executes and coordinates jobs\n",
    "\n",
    "This object is always used and provides __generic `spark` capabilities__\n",
    "\n",
    "### SQLContext\n",
    "\n",
    "> __Given `SparkContext` interact with `SparkSQL` library__\n",
    "\n",
    "One __had to__ provide `SparkContext` to this object in order to interact with SQL-like capabilities (e.g. creating `DataFrame`)\n",
    "\n",
    "### HiveContext\n",
    "\n",
    "> __Extension of SQLContext providing gateway to Hive__\n",
    "\n",
    "Hive is similar in structure to SQL but provides capabilities for data warehouse and is better suited for analyzing large scale data\n",
    "\n",
    "## SparkSession\n",
    "\n",
    "Above differentiation was pretty unsustainable and since `v2.0` of Spark \"one object to rule them all was introduced\" `spark.SparkSession`.\n",
    "\n",
    "In `pyspark` one can use it via `spark.sql.SparkSession` and it has the following capabilities:\n",
    "- __wraps functionalities of ALL CONTEXTS above__\n",
    "- we use a single object to interact with these APIs\n",
    "- __We should use `builder` attribute to obtain appropriate `SparkSession`__\n",
    "\n",
    "After `builder` attribute is run (which constructs appropriate context) one can use it just like `context` objects.\n",
    "\n",
    "Please note:\n",
    "- We use `config` to specify `SparkSession` configuration (essentially Spark configuration)\n",
    "- We use `getOrCreate()` which:\n",
    "    - If no global `Session` exists create a new one with specified config\n",
    "    - If global `Session` exists:\n",
    "        - Get an instance of it\n",
    "        - Apply new configuration to it\n",
    "    - This approach is save as __using multiple context is a bad practice!__ (although possible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d90bd58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:34:35.447266Z",
     "start_time": "2021-07-10T14:34:35.246044Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa53f8b",
   "metadata": {},
   "source": [
    "# Data Structures\n",
    "\n",
    "Before diving in we need to talk about `3` available data structures in `spark`:\n",
    "- `RDD` - __R__esilient __D__istributed __D__ataset - fault-tolerant collection of elements that can be operated on in parallel.\n",
    "- `DataFrame` -  dataset organised into named columns. Conceptually equivalent to a table in a relational database or a data frame in R/Python, __but with richer optimisations under the hood.__\n",
    "- `Dataset` - distributed collection of data. Provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine\n",
    "\n",
    "![](./images/rdd_df_dataset_history.png)\n",
    "\n",
    "# RDD & Core Spark API\n",
    "\n",
    "> __Core and basic of Spark applications with \"low-level\" operations__\n",
    "\n",
    "> __Fault-tolerant collection of elements that can be operated on in parallel.__\n",
    "\n",
    "This structure provides strong typing (via `JVM` objects) and can be constructued in two ways:\n",
    "- __parallelizing existing collection__ (e.g. Python's `list`)\n",
    "- __referencing dataset in external storage__ (anything compatible with Hadoop's InputFormat like HDFS, HBase, Amazon S3, text files etc.)\n",
    "\n",
    "Let's see these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bdde61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c7744",
   "metadata": {},
   "source": [
    "__Things to note for files__:\n",
    "- __Each file has to be in the same path on each worker node!__ (in our case we are running locally hence this is fine)\n",
    "- All file-based methods operate on:\n",
    "    - directories - `textFile(\"/my/directory\")`\n",
    "    - wildcards - `textFile(\"/my/directory/*.txt\")`\n",
    "    - compressed files - `textFile(\"/my/directory/*.gz\")`\n",
    "- We can change number of partitions created for this file\n",
    "- See API [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html#pyspark.SparkContext.textFile)\n",
    "\n",
    "> __Other ways to create RDD from file can be seen in [Spark Context API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#spark-context-apis), e.g. a way to create it from `pickle`__\n",
    "\n",
    "## Lazy Evaulation\n",
    "\n",
    "> Created RDDs __ARE NOT FILES__, they are merely a description of operation that __has to be run at some point__\n",
    "\n",
    "What we did above means:\n",
    "- Parallelize `list` operation\n",
    "- Read from text file `lorem.txt` (__but the read wasn't performed!__)\n",
    "\n",
    "> All of the operations will be run when we __request an ACTION__\n",
    "\n",
    "Actions may include:\n",
    "- return number of lines in file (whole map-reduce went through)\n",
    "- sum the list and return the result\n",
    "\n",
    "## Persist\n",
    "\n",
    "> Persisting is used in order to speed-up computations (saving intermediate results in memory)\n",
    "\n",
    "If we run the line below it means:\n",
    "\n",
    "> Read data file and cache read contents in the memory (if possible)\n",
    "\n",
    "> __If we run \"action\" on the file it will use the cached data (faster) rather than loading data from disk once again!__\n",
    "\n",
    "Rule of thumb: \n",
    "\n",
    "> Use cache when the lineage (operations to run on certain RDD) of your RDD branches out or when an RDD is used multiple times like in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aff9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the operations return self\n",
    "# This allows us to chain operations (we will see it in the next cell)\n",
    "\n",
    "rddDistributedFile = rddDistributedFile.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614526c4",
   "metadata": {},
   "source": [
    "> __`.cache()` is the same as `.persist()` with `StorageLevel.MEMORY_ONLY`__\n",
    "\n",
    "There are few other options to store the data:\n",
    "- `MEMORY_ONLY` - keep everything we can in memory otherwise do not cache and compute results\n",
    "- `MEMORY_AND_DISK` - keep everything we can in memory otherwise serialize to disk (__encouraged for long running computations we would like to cache__)\n",
    "- `DISK_ONLY` - cache everything on disk, nothing in memory (__discouraged__)\n",
    "- `MEMORY_ONLY_2` - same as `MEMORY_ONLY` but replicates cache on two cluster nodes for improved fault tolerance (`DISK_ONLY_2` is also available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "666b62a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, False, False, False, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.StorageLevel.DISK_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035bfe3",
   "metadata": {},
   "source": [
    "## MapReduce operations\n",
    "\n",
    "> Given parallelized data structure we can run map-reduce operations on it\n",
    "\n",
    "All of them can be seen [in the documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis), a few interesting ones:\n",
    "- [`rdd.checkpoint()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.checkpoint.html#pyspark.RDD.checkpoint) - will be saved in checkpoint directory and all the operations creating it __are discarded__ (action)\n",
    "- [`rdd.collect()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect) - __return the structure__ (collect it after operations) (action)\n",
    "- [`rdd.count()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html#pyspark.RDD.count) - count elements in the structure (action)\n",
    "- [`rdd.countByKey()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByKey.html#pyspark.RDD.countByKey) - count number of elements for each `key` in `(key, value)` pairs (similar to what the graphic before did)\n",
    "- [`rdd.countByValue()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByValue.html#pyspark.RDD.countByValue) - count __how many unique values__ are in this structure (returned as `(value, count)` dictionary)\n",
    "\n",
    "__And the essential ones we will use are:__\n",
    "- [`rdd.map(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map) - apply function __to each element in the collection__\n",
    "- [`rdd.filter(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html#pyspark.RDD.filter) - __choose values which fulfill `f` function__\n",
    "- [`rdd.flatMap(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html#pyspark.RDD.flatMap) - __apply function to each element and `flatten` the list if necessary__\n",
    "- [`rdd.fold(neutralValue, f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.fold.html#pyspark.RDD.fold) - __given associative function (like `add`) takes every 2 elements together and returns the result__\n",
    "- [`rdd.sortBy(keyfunction)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html#pyspark.RDD.sortBy) - sort by specific function which returns some value from the `(key, value)` pair\n",
    "\n",
    "> __PLEASE REFER TO DOCUMENTATION WHEN LOOKING FOR AN OPERATOR! MANY OF THEM ARE ALREADY IMPLEMENTED!__\n",
    "\n",
    "> __TAKE TIME TO COME UP WITH THE OPERATORS NEEDED! EACH OPERATION SAVED MIGHT IMPROVE RUNTIME TREMENDOUSLY!__\n",
    "\n",
    "Let's see an example chaining on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3051cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc is standard name for sparkContext\n",
    "# it will be easier to use from now on\n",
    "\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c63be368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "data = list(range(10,-11,-1))\n",
    "print(data)\n",
    "\n",
    "result = (\n",
    "    sc.parallelize(data)\n",
    "    .filter(lambda val: val % 3 == 0)\n",
    "    .map(operator.abs)\n",
    "    .fold(0, operator.add)\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60fbd933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([\"b\", \"a\", \"c\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fd240d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Lorem': 3,\n",
       "             'ipsum': 11,\n",
       "             'dolor': 10,\n",
       "             'sit': 50,\n",
       "             'amet,': 1,\n",
       "             'consectetur': 12,\n",
       "             'adipiscing': 23,\n",
       "             'elit,': 1,\n",
       "             'sed': 66,\n",
       "             'do': 1,\n",
       "             'eiusmod': 1,\n",
       "             'tempor': 11,\n",
       "             'incididunt': 1,\n",
       "             'ut': 48,\n",
       "             'labore': 1,\n",
       "             'et': 34,\n",
       "             'dolore': 1,\n",
       "             'magna': 15,\n",
       "             'aliqua.': 1,\n",
       "             'Quam': 5,\n",
       "             'lacus': 21,\n",
       "             'suspendisse': 17,\n",
       "             'faucibus': 25,\n",
       "             'interdum': 12,\n",
       "             'posuere.': 3,\n",
       "             'Dui': 5,\n",
       "             'accumsan': 10,\n",
       "             'amet': 53,\n",
       "             'nulla': 31,\n",
       "             'facilisi': 11,\n",
       "             'morbi': 25,\n",
       "             'tempus.': 2,\n",
       "             'Lobortis': 4,\n",
       "             'scelerisque': 23,\n",
       "             'fermentum': 17,\n",
       "             'dui': 21,\n",
       "             'in': 48,\n",
       "             'ornare': 16,\n",
       "             'quam': 24,\n",
       "             'viverra.': 6,\n",
       "             'In': 10,\n",
       "             'viverra': 39,\n",
       "             'orci': 21,\n",
       "             'sagittis': 19,\n",
       "             'eu.': 6,\n",
       "             'Urna': 1,\n",
       "             'duis': 10,\n",
       "             'convallis': 13,\n",
       "             'tellus': 28,\n",
       "             'id': 47,\n",
       "             'interdum.': 5,\n",
       "             'A': 5,\n",
       "             'erat': 7,\n",
       "             'nam': 5,\n",
       "             'at': 36,\n",
       "             'lectus.': 5,\n",
       "             'Eget': 9,\n",
       "             'lorem': 11,\n",
       "             'Scelerisque': 5,\n",
       "             'eu': 30,\n",
       "             'ultrices': 18,\n",
       "             'vitae': 33,\n",
       "             'auctor': 15,\n",
       "             'augue': 21,\n",
       "             'ut.': 11,\n",
       "             'Orci': 3,\n",
       "             'porta': 8,\n",
       "             'non': 26,\n",
       "             'pulvinar': 18,\n",
       "             'neque': 18,\n",
       "             'laoreet': 10,\n",
       "             'libero.': 1,\n",
       "             'Egestas': 7,\n",
       "             'rutrum': 3,\n",
       "             'pellentesque': 33,\n",
       "             'tincidunt': 22,\n",
       "             'tortor': 21,\n",
       "             'aliquam.': 8,\n",
       "             'Vivamus': 2,\n",
       "             'eget': 36,\n",
       "             'arcu.': 8,\n",
       "             'Id': 9,\n",
       "             'leo': 16,\n",
       "             'turpis.': 5,\n",
       "             'Rutrum': 2,\n",
       "             'aliquam': 24,\n",
       "             'nulla.': 2,\n",
       "             'Mauris': 3,\n",
       "             'a': 24,\n",
       "             'diam': 25,\n",
       "             'maecenas': 14,\n",
       "             'enim': 45,\n",
       "             'urna.': 5,\n",
       "             'lectus': 20,\n",
       "             'est': 16,\n",
       "             'placerat': 8,\n",
       "             'egestas.': 6,\n",
       "             'Non': 3,\n",
       "             'curabitur': 4,\n",
       "             'gravida': 19,\n",
       "             'arcu': 24,\n",
       "             'ac': 20,\n",
       "             'dignissim': 12,\n",
       "             'convallis.': 4,\n",
       "             'Proin': 3,\n",
       "             'libero': 10,\n",
       "             'nunc': 30,\n",
       "             'consequat': 11,\n",
       "             'varius': 8,\n",
       "             'sit.': 10,\n",
       "             'Sit': 12,\n",
       "             'commodo': 15,\n",
       "             'nullam': 8,\n",
       "             'vehicula': 2,\n",
       "             'a.': 5,\n",
       "             'Eleifend': 1,\n",
       "             'proin': 15,\n",
       "             'nisl': 24,\n",
       "             'rhoncus.': 2,\n",
       "             'Elementum': 5,\n",
       "             'etiam': 10,\n",
       "             'lacus.': 3,\n",
       "             'Cursus': 6,\n",
       "             'mauris': 22,\n",
       "             'Senectus': 1,\n",
       "             'netus': 10,\n",
       "             'malesuada': 17,\n",
       "             'fames': 5,\n",
       "             'turpis': 22,\n",
       "             'suspendisse.': 7,\n",
       "             'Massa': 5,\n",
       "             'egestas': 37,\n",
       "             'purus': 19,\n",
       "             'in.': 9,\n",
       "             'sodales': 4,\n",
       "             'amet.': 8,\n",
       "             'condimentum': 10,\n",
       "             'lacinia': 5,\n",
       "             'quis': 27,\n",
       "             'vel': 19,\n",
       "             'eros': 4,\n",
       "             'donec.': 2,\n",
       "             'Magna': 3,\n",
       "             'blandit.': 2,\n",
       "             'Mattis': 7,\n",
       "             'ullamcorper': 20,\n",
       "             'velit': 20,\n",
       "             'massa': 25,\n",
       "             'eget.': 10,\n",
       "             'Morbi': 7,\n",
       "             'tristique': 22,\n",
       "             'senectus': 12,\n",
       "             'Laoreet': 3,\n",
       "             'tincidunt.': 7,\n",
       "             'risus': 20,\n",
       "             'mi': 19,\n",
       "             'Fames': 1,\n",
       "             'pharetra': 14,\n",
       "             'Ultrices': 7,\n",
       "             'cursus': 20,\n",
       "             'aliquet': 19,\n",
       "             'nibh': 20,\n",
       "             'condimentum.': 3,\n",
       "             'Ipsum': 4,\n",
       "             'dictum': 13,\n",
       "             'fusce.': 1,\n",
       "             'Tempus': 2,\n",
       "             'nec': 17,\n",
       "             'nam.': 1,\n",
       "             'Parturient': 1,\n",
       "             'montes': 2,\n",
       "             'nascetur': 2,\n",
       "             'ridiculus': 1,\n",
       "             'mus': 2,\n",
       "             'ultricies.': 2,\n",
       "             'odio': 20,\n",
       "             'Amet': 8,\n",
       "             'tempus': 14,\n",
       "             'iaculis': 12,\n",
       "             'urna': 26,\n",
       "             'volutpat.': 6,\n",
       "             'Eu': 7,\n",
       "             'bibendum': 17,\n",
       "             'pharetra.': 4,\n",
       "             'Vitae': 6,\n",
       "             'rhoncus': 10,\n",
       "             'mattis': 19,\n",
       "             'elit': 21,\n",
       "             'pellentesque.': 9,\n",
       "             'hendrerit': 5,\n",
       "             'quisque': 6,\n",
       "             'auctor.': 3,\n",
       "             'Vel': 4,\n",
       "             'pulvinar.': 3,\n",
       "             'Dolor': 1,\n",
       "             'aliquet.': 4,\n",
       "             'Tortor': 2,\n",
       "             'venenatis': 10,\n",
       "             'cras': 10,\n",
       "             'felis.': 1,\n",
       "             'Nisl': 3,\n",
       "             'pretium': 21,\n",
       "             'fusce': 6,\n",
       "             'Nisi': 3,\n",
       "             'mollis': 6,\n",
       "             'Ornare': 2,\n",
       "             'nisl.': 3,\n",
       "             'Suspendisse': 1,\n",
       "             'nisi': 9,\n",
       "             'hac.': 1,\n",
       "             'dui.': 5,\n",
       "             'Imperdiet': 2,\n",
       "             'iaculis.': 6,\n",
       "             'Arcu': 3,\n",
       "             'euismod': 11,\n",
       "             'nunc.': 7,\n",
       "             'Viverra': 9,\n",
       "             'cras.': 4,\n",
       "             'Pellentesque': 6,\n",
       "             'volutpat': 22,\n",
       "             'habitant': 9,\n",
       "             'tristique.': 1,\n",
       "             'quam.': 6,\n",
       "             'Sed': 8,\n",
       "             'ultricies': 16,\n",
       "             'scelerisque.': 3,\n",
       "             'suscipit': 7,\n",
       "             'integer.': 4,\n",
       "             'fringilla': 8,\n",
       "             'porttitor': 7,\n",
       "             'enim.': 3,\n",
       "             'Enim': 10,\n",
       "             'vulputate': 21,\n",
       "             'Sapien': 3,\n",
       "             'ligula': 1,\n",
       "             'proin.': 2,\n",
       "             'Ullamcorper': 2,\n",
       "             'varius.': 2,\n",
       "             'Condimentum': 2,\n",
       "             'ac.': 2,\n",
       "             'Diam': 11,\n",
       "             'sem': 8,\n",
       "             'Ac': 7,\n",
       "             'posuere': 11,\n",
       "             'morbi.': 8,\n",
       "             'facilisis': 12,\n",
       "             'mauris.': 6,\n",
       "             'tortor.': 4,\n",
       "             'semper': 14,\n",
       "             'diam.': 4,\n",
       "             'vivamus': 6,\n",
       "             'felis': 11,\n",
       "             'Dignissim': 1,\n",
       "             'integer': 16,\n",
       "             'vitae.': 5,\n",
       "             'Integer': 5,\n",
       "             'feugiat': 12,\n",
       "             'Tellus': 7,\n",
       "             'id.': 7,\n",
       "             'Nibh': 4,\n",
       "             'odio.': 2,\n",
       "             'Adipiscing': 6,\n",
       "             'sollicitudin': 6,\n",
       "             'nibh.': 4,\n",
       "             'Cras': 2,\n",
       "             'consectetur.': 2,\n",
       "             'Aliquet': 8,\n",
       "             'luctus': 2,\n",
       "             'Consequat': 2,\n",
       "             'justo': 10,\n",
       "             'laoreet.': 2,\n",
       "             'Posuere': 1,\n",
       "             'molestie.': 3,\n",
       "             'Tempor': 3,\n",
       "             'mi.': 2,\n",
       "             'Velit': 2,\n",
       "             'Sem': 3,\n",
       "             'Congue': 2,\n",
       "             'aenean': 7,\n",
       "             'elementum': 24,\n",
       "             'sagittis.': 2,\n",
       "             'ipsum.': 5,\n",
       "             'pretium.': 4,\n",
       "             'Sollicitudin': 1,\n",
       "             'luctus.': 1,\n",
       "             'Est': 5,\n",
       "             'facilisi.': 1,\n",
       "             'Ut': 9,\n",
       "             'blandit': 11,\n",
       "             'erat.': 5,\n",
       "             'Donec': 3,\n",
       "             'sapien': 7,\n",
       "             'malesuada.': 3,\n",
       "             'bibendum.': 2,\n",
       "             'adipiscing.': 3,\n",
       "             'sed.': 13,\n",
       "             'dignissim.': 3,\n",
       "             'Leo': 2,\n",
       "             'molestie': 11,\n",
       "             'elementum.': 2,\n",
       "             'Risus': 5,\n",
       "             'lobortis': 8,\n",
       "             'faucibus.': 4,\n",
       "             'phasellus': 8,\n",
       "             'vestibulum': 13,\n",
       "             'Etiam': 2,\n",
       "             'Vestibulum': 4,\n",
       "             'ullamcorper.': 3,\n",
       "             'Sagittis': 5,\n",
       "             'Maecenas': 1,\n",
       "             'nisi.': 1,\n",
       "             'Convallis': 2,\n",
       "             'neque.': 4,\n",
       "             'porta.': 3,\n",
       "             'Aenean': 4,\n",
       "             'eleifend': 8,\n",
       "             'Porttitor': 2,\n",
       "             'risus.': 5,\n",
       "             'Elit': 5,\n",
       "             'imperdiet': 11,\n",
       "             'ante': 2,\n",
       "             'mattis.': 5,\n",
       "             'metus': 7,\n",
       "             'Bibendum': 3,\n",
       "             'At': 12,\n",
       "             'Fusce': 2,\n",
       "             'Tristique': 4,\n",
       "             'Lacus': 4,\n",
       "             'lacinia.': 1,\n",
       "             'Pretium': 7,\n",
       "             'donec': 11,\n",
       "             'Feugiat': 3,\n",
       "             'praesent': 7,\n",
       "             'feugiat.': 3,\n",
       "             'Dictum': 3,\n",
       "             'Et': 3,\n",
       "             'Malesuada': 2,\n",
       "             'accumsan.': 3,\n",
       "             'Faucibus': 4,\n",
       "             'quis.': 5,\n",
       "             'aenean.': 2,\n",
       "             'cursus.': 4,\n",
       "             'Pharetra': 4,\n",
       "             'Vulputate': 3,\n",
       "             'Augue': 2,\n",
       "             'Nec': 2,\n",
       "             'hendrerit.': 1,\n",
       "             'Libero': 1,\n",
       "             'dictum.': 3,\n",
       "             'orci.': 5,\n",
       "             'rutrum.': 2,\n",
       "             'hac': 1,\n",
       "             'habitasse': 1,\n",
       "             'platea': 1,\n",
       "             'dictumst': 1,\n",
       "             'vestibulum.': 3,\n",
       "             'Ante': 1,\n",
       "             'Fermentum': 1,\n",
       "             'ornare.': 6,\n",
       "             'Mi': 4,\n",
       "             'congue': 4,\n",
       "             'potenti.': 1,\n",
       "             'Nunc': 5,\n",
       "             'purus.': 3,\n",
       "             'facilisis.': 2,\n",
       "             'Molestie': 1,\n",
       "             'Tincidunt': 4,\n",
       "             'venenatis.': 4,\n",
       "             'Volutpat': 2,\n",
       "             'tellus.': 3,\n",
       "             'Turpis': 3,\n",
       "             'Penatibus': 1,\n",
       "             'magnis': 1,\n",
       "             'dis': 1,\n",
       "             'parturient': 1,\n",
       "             'ridiculus.': 1,\n",
       "             'Commodo': 3,\n",
       "             'non.': 4,\n",
       "             'Purus': 2,\n",
       "             'fermentum.': 4,\n",
       "             'Venenatis': 2,\n",
       "             'Lectus': 2,\n",
       "             'porttitor.': 5,\n",
       "             'vulputate.': 1,\n",
       "             'Facilisis': 1,\n",
       "             'elit.': 3,\n",
       "             'et.': 10,\n",
       "             'Auctor': 2,\n",
       "             'Euismod': 1,\n",
       "             'at.': 8,\n",
       "             'Justo': 1,\n",
       "             'phasellus.': 2,\n",
       "             'Curabitur': 1,\n",
       "             'senectus.': 3,\n",
       "             'potenti': 1,\n",
       "             'Neque': 2,\n",
       "             'placerat.': 2,\n",
       "             'cum': 2,\n",
       "             'sociis': 2,\n",
       "             'natoque.': 2,\n",
       "             'Nulla': 3,\n",
       "             'Odio': 2,\n",
       "             'Dictumst': 2,\n",
       "             'Duis': 1,\n",
       "             'Blandit': 3,\n",
       "             'lorem.': 3,\n",
       "             'imperdiet.': 2,\n",
       "             'Aliquam': 2,\n",
       "             'Ridiculus': 1,\n",
       "             'vel.': 2,\n",
       "             'Porta': 2,\n",
       "             'duis.': 2,\n",
       "             'Eros': 3,\n",
       "             'gravida.': 3,\n",
       "             'euismod.': 1,\n",
       "             'Suscipit': 1,\n",
       "             'Accumsan': 1,\n",
       "             'Pulvinar': 2,\n",
       "             'semper.': 1,\n",
       "             'dapibus': 2,\n",
       "             'ultrices.': 2,\n",
       "             'fringilla.': 2,\n",
       "             'ante.': 1,\n",
       "             'Luctus': 1,\n",
       "             'Quis': 1,\n",
       "             'Ultricies': 2,\n",
       "             'Placerat': 1,\n",
       "             'Potenti': 1,\n",
       "             'maecenas.': 2,\n",
       "             'congue.': 1,\n",
       "             'Dapibus': 1,\n",
       "             'Metus': 1,\n",
       "             'augue.': 1,\n",
       "             'est.': 2,\n",
       "             'massa.': 1,\n",
       "             'Nam': 1,\n",
       "             'Habitant': 1,\n",
       "             'netus.': 2,\n",
       "             'fames.': 1,\n",
       "             'Mollis': 1,\n",
       "             'leo.': 2,\n",
       "             'Consectetur': 1,\n",
       "             'Semper': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddDistributedFile.flatMap(lambda text: text.split()).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069261a",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "## Dataset and DataFrame\n",
    "\n",
    "Dataset is a distributed collection of data which provides:\n",
    "- strong typing and powerful lambda functions from `RDD`\n",
    "- __allows for Spark SQL optimized execution engine__\n",
    "\n",
    "It can be created from JVM objects __and manipulated in the same functional manner__.\n",
    "\n",
    "> __`pyspark` has no Dataset API but many benefits of `Dataset` are available for `DataFrame`s DUE TO IT'S DYNAMIC NATURE__\n",
    "\n",
    "DataFrame shortcomings included:\n",
    "- No compile-time safety, hence __you cannot manipulate data of which structure is not specified__\n",
    "\n",
    "> DataFrame is a a  Dataset organized into named columns (__same as for `pd.DataFrame`__)\n",
    "\n",
    "From now on we will use `DataFrame`s (__not `Dataset`, also due to Python's community similarity with `pd.DataFrame`__) to keep our records.\n",
    "\n",
    "See [this discussion](https://stackoverflow.com/questions/31508083/difference-between-dataframe-dataset-and-rdd-in-spark) for an extended description.\n",
    "\n",
    "## Creating DataFrames\n",
    "\n",
    "> __For all of the operations we can use `SparkSession` directly to interact with the cluster!__\n",
    "\n",
    "There are a few options usable for us to read data residing on clusters (__for each node it has to be at the same location if reading from file!__):\n",
    "- [`session.createDataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html#pyspark.sql.SparkSession.createDataFrame) - create `pyspark.sql.DataFrame` from:\n",
    "    - `RDD`\n",
    "    - `list`\n",
    "    - `pandas.DataFrame`\n",
    "    - __Optionally: with `schema`__ which specifies datatypes and format for data contained within it. See documentation for more info.\n",
    "    - By default `schema` is inferred if possible\n",
    "- [`session.range`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.range.html#pyspark.sql.SparkSession.range) - works like Python's range but distributed and as a `spark.DataFrame`\n",
    "- [`session.sql(query)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql) - __return DataFrame which represents result of `sql` query__\n",
    "- [`session.read.{how_to_read}()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) - __returns `DataFrameReader` object__ which allows us to read `df` from:\n",
    "    - `json`\n",
    "    - `parquet`\n",
    "    - `csv`\n",
    "    - and many more\n",
    "- [`session.readStream`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.readStream.html#pyspark.sql.SparkSession.readStream) - __used for streaming, we will see it a little later__\n",
    "\n",
    "Let's see some code with `pyspark.sql.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d903104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "| 10|  1| 63| 97|\n",
      "| 15| 84| 33| 85|\n",
      "| 50| 33| 73|  9|\n",
      "| 49|  3| 12| 52|\n",
      "| 49|  4| 35| 51|\n",
      "| 31| 28| 76| 42|\n",
      "| 99|  5| 49| 19|\n",
      "| 71| 66| 68| 50|\n",
      "| 45| 48|  7| 32|\n",
      "| 92| 37| 84| 19|\n",
      "| 40| 53| 63| 30|\n",
      "| 43| 22| 64| 46|\n",
      "| 66| 49|  6| 67|\n",
      "| 20| 41| 21| 71|\n",
      "| 40| 93| 81|  9|\n",
      "| 50| 78|  6| 31|\n",
      "| 64| 62| 18| 43|\n",
      "| 52| 33| 37| 50|\n",
      "| 75| 88| 65| 82|\n",
      "| 90|  8| 87|  5|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = session.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        np.random.randint(0, 100, size=(100, 4)),\n",
    "        columns=list(\"ABCD\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e38c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: long (nullable = true)\n",
      " |-- B: long (nullable = true)\n",
      " |-- C: long (nullable = true)\n",
      " |-- D: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a1468c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  A|\n",
      "+---+\n",
      "| 10|\n",
      "| 15|\n",
      "| 50|\n",
      "| 49|\n",
      "| 49|\n",
      "| 31|\n",
      "| 99|\n",
      "| 71|\n",
      "| 45|\n",
      "| 92|\n",
      "| 40|\n",
      "| 43|\n",
      "| 66|\n",
      "| 20|\n",
      "| 40|\n",
      "| 50|\n",
      "| 64|\n",
      "| 52|\n",
      "| 75|\n",
      "| 90|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show is an action, nothing would be returned without it\n",
    "# Just an operation representing what will happen\n",
    "df.select(\"A\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e40cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[A: bigint, (B + 1): bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df[\"A\"], df[\"B\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785ba5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|  A|(B + 1)|\n",
      "+---+-------+\n",
      "| 10|      2|\n",
      "| 15|     85|\n",
      "| 50|     34|\n",
      "| 49|      4|\n",
      "| 49|      5|\n",
      "| 31|     29|\n",
      "| 99|      6|\n",
      "| 71|     67|\n",
      "| 45|     49|\n",
      "| 92|     38|\n",
      "| 40|     54|\n",
      "| 43|     23|\n",
      "| 66|     50|\n",
      "| 20|     42|\n",
      "| 40|     94|\n",
      "| 50|     79|\n",
      "| 64|     63|\n",
      "| 52|     34|\n",
      "| 75|     89|\n",
      "| 90|      9|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Increase column value by one\n",
    "# This operation is shown in the output\n",
    "\n",
    "df.select(df[\"A\"], df[\"B\"] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33de5702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  B|count|\n",
      "+---+-----+\n",
      "| 26|    3|\n",
      "| 84|    2|\n",
      "| 98|    2|\n",
      "| 71|    2|\n",
      "|  6|    2|\n",
      "| 27|    2|\n",
      "| 51|    3|\n",
      "| 41|    2|\n",
      "| 33|    2|\n",
      "| 28|    3|\n",
      "| 88|    3|\n",
      "| 48|    5|\n",
      "| 44|    2|\n",
      "|  3|    3|\n",
      "| 37|    2|\n",
      "| 62|    3|\n",
      "| 59|    2|\n",
      "| 15|    2|\n",
      "| 38|    2|\n",
      "| 46|    4|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counted = df.groupby(\"B\").count().persist()\n",
    "counted.filter(counted[\"count\"] > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81011b59",
   "metadata": {},
   "source": [
    "## Operations on DataFrame\n",
    "\n",
    "> __`pyspark.sql.DataFrame` supports most of the `pd.DataFrame` operations + the RDD ones__\n",
    "\n",
    "You can see the whole list [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)\n",
    "\n",
    "> __In general one can work with it similarly to how one works with `pd.DataFrame` objects__\n",
    "\n",
    "there are a few exceptions though...\n",
    "\n",
    "## Running SQL queries\n",
    "\n",
    "> In order to run SQL queries against the DataFrame __we have to register them as `TemporaryViews`__\n",
    "\n",
    "Properties of `TemporaryViews`:\n",
    "- __Session scoped__ - if session runs out of scope so will the views registered for it\n",
    "- One can set up `DataFrame` globally for any `SparkSession` by using `df.createGlobalTempView(\"name_of_database\")`\n",
    "\n",
    "After that, we can run SQL queries against __distributed data across nodes__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "420709b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "| 10|  1| 63| 97|\n",
      "| 15| 84| 33| 85|\n",
      "| 50| 33| 73|  9|\n",
      "| 49|  3| 12| 52|\n",
      "| 49|  4| 35| 51|\n",
      "| 31| 28| 76| 42|\n",
      "| 99|  5| 49| 19|\n",
      "| 71| 66| 68| 50|\n",
      "| 45| 48|  7| 32|\n",
      "| 92| 37| 84| 19|\n",
      "| 40| 53| 63| 30|\n",
      "| 43| 22| 64| 46|\n",
      "| 66| 49|  6| 67|\n",
      "| 20| 41| 21| 71|\n",
      "| 40| 93| 81|  9|\n",
      "| 50| 78|  6| 31|\n",
      "| 64| 62| 18| 43|\n",
      "| 52| 33| 37| 50|\n",
      "| 75| 88| 65| 82|\n",
      "| 90|  8| 87|  5|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"any_name\")\n",
    "\n",
    "# WE USE SESSION TO RUN QUERIES!\n",
    "sqlDf = session.sql(\"SELECT * FROM any_name\")\n",
    "sqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd8677",
   "metadata": {},
   "source": [
    "# Spark-Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673c112",
   "metadata": {},
   "source": [
    "The work you see in this notebook sent applications to a clusted interactively, meaning that you were running all cells sequentially. \n",
    "\n",
    "In a production environment, you are more likely to launch the applications from a script, where that script contains all the operations using PySpark. \n",
    "\n",
    "To do so, you can use spark-submit, which can be ran from the terminal to _submit_ your Spark applications. The syntax is as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c275d",
   "metadata": {},
   "source": [
    "```\n",
    "spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac83e21",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "- __class__ is the entrypoint for your application\n",
    "- __master__ the URL of your cluster. You can set it to `local` to run it locally\n",
    "- __deploy-mode__ Whether to deploy on the worker or locally as a client\n",
    "- __conf__ Configuration of the Spark application in a `key=value` way\n",
    "- __application-jar__: Path to a your application\n",
    "\n",
    "Within other options, you can specify number of workers or the number of cores:\n",
    "\n",
    "- __--num-executors__\n",
    "- __--num-cores__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73cc55",
   "metadata": {},
   "source": [
    "In this case, we are going to submit the same example we were working with. This application will print put the words in lorem, and the number of occurences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.py\n",
    "\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    # create Spark context with Spark configuration\n",
    "    conf = SparkConf().setAppName(\"Word Count - Python\").setMaster('local[*]')\n",
    "    session = pyspark.sql.SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "    # read in text file and split each document into words\n",
    "    rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
    "    rddDistributedFile = rddDistributedFile.cache()\n",
    "    # count the occurrence of each word\n",
    "    print(rddDistributedFile.flatMap(lambda text: text.split()).countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d8610",
   "metadata": {},
   "source": [
    "In this repo, you will find a `example.py` files that you can try for submitting your application. You can run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5af65",
   "metadata": {},
   "source": [
    "`<SPARK_HOME>/bin/spark-submit.cmd example.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decf223",
   "metadata": {},
   "source": [
    "If you encounter an error, you might need to paste a file `winutils.exe` for running the command above. You can download the corresponding version [here](https://github.com/steveloughran/winutils).\n",
    "\n",
    "Your directory should look like this:\n",
    "\n",
    "```\n",
    "~/\n",
    "│\n",
    "├── spark/\n",
    "│   └── spark-3.1.2-bin-hadoop3.2  <--- SPARK_HOME\n",
    "│         ├── bin\n",
    "│         ├── conf\n",
    "│         ├── data\n",
    "│         ├── examples\n",
    "│         ├── hadoop               <--- Add this new folder = HADOOP_HOME\n",
    "│         │    └── bin\n",
    "│         │         └── winutils.exe\n",
    "... \n",
    "```\n",
    "\n",
    "Then, you'll have to set the a new environment variable `HADOOP_HOME` with the directory of the folder `hadoop`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ca66b",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- Check out [`rdd.aggregate`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.aggregate.html#pyspark.RDD.aggregate) method for RDDs.\n",
    "- What is the difference between `forEach` and `map`? Check [this StackOverflow answer](https://stackoverflow.com/questions/354909/is-there-a-difference-between-foreach-and-map) if in doubt\n",
    "- What is the difference between `reduce` and `fold`? check [this StackOverflow answer](https://stackoverflow.com/a/36060141/10886420). Which one is \"safer\" to use?\n",
    "- Which operations on RDDs induce `shuffle` and why is it a problem? See [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations) for more info\n",
    "- Check how to use [Hive](https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html) with PySpark. What Hive is and how it differs from SQL?\n",
    "- Check out how to specify schema programmaticaly (presented [in this tutorial](https://spark.apache.org/docs/latest/sql-getting-started.html#programmatically-specifying-the-schema)). What are the upsides/downsides of using it?\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- Read more about multiple `SparkContext`s and `SparkSession`s and why would we need it in some... contexts. Check it [over here](https://www.waitingforcode.com/apache-spark-sql/multiple-sparksession-one-sparkcontext/read)\n",
    "- What is [`rdd.meanApprox`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.meanApprox.html#pyspark.RDD.meanApprox) and why might we need it?\n",
    "- Generally discouraged, but what are the options to share data between tasks and nodes in the cluster? Check out [this part of RDD tutorial](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables)\n",
    "- Check [performance tuning options for `spark.sql`](https://spark.apache.org/docs/latest/sql-performance-tuning.html). One can use them when creating `pyspark.SparkConf()` object"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6971ff02672853a145ab8a619e17e4c2b989e1ba4684228133b86b474ce57f92"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
