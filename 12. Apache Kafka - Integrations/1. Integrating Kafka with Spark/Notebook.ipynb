{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Integrating Kafka with Apache Spark </h1>\n",
    "\n",
    "> One of the most popular use cases in industry is to connect Kafka with Apache Spark, whereby Kafka feeds data into Spark which implements data processing.\n",
    "\n",
    "In large global organizations, Kafka is not used in isolation by itself, but rather it's usually part of a larger data system that involves multiple steps and phases.\n",
    "\n",
    "As a quick reminder, Apache Spark is a fast and efficient unified data analytics engine for Big Data and machine learning.  If you haven't already done so, check out the AiCore Spark content for more details.\n",
    "\n",
    "Below is a high-level overview of the Apache Spark ecosystem:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/spark-ecosystem.png\" width=600>\n",
    "  </p>\n",
    "\n",
    "For the purpose of this lesson, we'll be focusing on __Spark Streaming__.\n",
    "\n",
    "If you are unfamiliar with Spark, pleasure ensure to review the Spark module.\n",
    "\n",
    "\n",
    "To get more detailed information about Spark, you can also read the below link\n",
    "  - __[What is Apache Spark](https://www.ibm.com/cloud/learn/apache-spark)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka and Apache Spark\n",
    "\n",
    "It is common in global companies to use both Kafka and Spark as part of a larger data processing ecosystem.\n",
    "\n",
    "Below is an example of how such a system would typically look like:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/kafka-spark.png\" width=600>\n",
    "  </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that, in such a system, we have multiple phases or steps in the data flow.  Typically, the flow would be as follows:\n",
    "\n",
    "__1. Data Streaming:__\n",
    "- This is where the raw data is normally produced. \n",
    "-  It could be either real-time data (such as machine/IoT generated data) or batch data (such as manually entered data into a system).\n",
    "\n",
    "<p></p>\n",
    "\n",
    "__2. Data Collection:__  \n",
    "- The data would then go through a Kafka system, which will be reponsible to connect to the data producer on one hand, and the data consumer (in this case Apache Spark) on the other hand.\n",
    "-  Kafka also buffers the data until its consumed by Spark.\n",
    "<p></p>\n",
    "\n",
    "__2. Data Processing__\n",
    "- In this topology, Spark (in general) or the Spark Streaming module in particular would be the data consumer, as it'll be on the recieving end of Kafka.\n",
    "- Data arriving into Spark can be analyzed and processed on the fly in real-time (batch processing is also supported).\n",
    "- Post-processed data can then be sent to a persistent storage layer where it will be kept for downstream processing.\n",
    "<p></p>\n",
    "\n",
    "__3. Data Storage__\n",
    "- The data storage layer is where the post-processed information is stored.\n",
    "- There are several options to chose from, including Hadoop HDFS, Amazon S3, NoSQL data stores such as HBase or MongoDB or other commodity hard drives among others.  It's also possible to produce the data to a seperate Kafka topic which can be connected to other downstream systems.\n",
    "- The cleaned, formatted and processed data will then be accessed by downstream systems that perform various analytics and data science work.\n",
    "<p></p>\n",
    "\n",
    "__4. Data Analysis/Visualization__\n",
    "- In this layer of the process, the prepared data is fed to front-end systems that are used for business intelligence, analytics and data science purposes.\n",
    "- Data is connected to dashboards and visualization tools which can be leveraged to create reports for top-level business executives.\n",
    "- Data Science teams use the preapred information to train and test their models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how Shopify use Kafka, Python and Spark Streaming for real-time risk management in this [video](https://databricks.com/session/realtime-risk-management-using-kafka-python-and-spark-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to implement Kafka-Spark integration\n",
    "\n",
    "Now that we have a solid understanding of Kafka, Spark and how both are used together in the real-world, it's time to roll-up our sleeves and start coding.\n",
    "\n",
    "> In this tutorial, Kafka will act as the streaming platform to produce and consumer events to the Spark framework, while Spark will be used as the data processing engine to count the words from each message batch recieved from Kafka.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high-level, the steps involved to implement this excercise include:\n",
    "\n",
    "1. Setting up the Kafka environment\n",
    "2. Creating a new Kafka topic\n",
    "3. Setting up the Spark environment\n",
    "4. Create/use Wordcount Spark Streaming Python code \n",
    "5. Run the Wordcount application\n",
    "6. Open a Kafka producer \n",
    "7. Print the output as the data arrives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up the Kafka environment\n",
    "\n",
    "> Ensure that you've followed the detailed instructions in the Kafka module - to properly setup your Kafka environment.\n",
    "\n",
    "As usual, we'll need to start the Kafka server and zookeeper first. To do so, run the following commands in the terminal (ensure you're in the Kafka folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Kafka Server\n",
    "bash bin/kafka-server-start.sh config/server.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Zookeeper Server\n",
    "bash bin/zookeeper-server-start.sh config/zookeeper.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming this ran successfully, you should see output similar to:\n",
    "\n",
    "![](images/kafka-zookeeper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating a new Kafka topic\n",
    "\n",
    "In order to be able to use Kafka, we'll need a topic to store the data.  To create a new topic, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Wordcount topic\n",
    "kafka-topics --create --zookeeper zookeeper_server:2181 --topic wordcounttopic --partitions 1 --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setting up the Spark environment\n",
    "\n",
    "> Make sure to have followed the steps in the Spark lessons to setup Spark locally on your machine.  If you haven't yet done so, please go to the __Spark Basics__ and __Spark Streaming__ notebooks and follow the steps there before proceeding.\n",
    "\n",
    "We'll also need to have PySpark installed to be able to run this application.  To check if its already installed, run the following command in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If PySpark is properly setup, you should see output similar to the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark-download-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If PySpark is not setup, you can install it using the following `Pip` command (also, please refer to the Spark Basics and Spark Streaming notebooks for full instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Wordcount Spark Streaming Python code\n",
    "\n",
    "We need to create a Wordcount application that divides data input streams into batches of 10 seconds, and then counts occurances of the words in each batch.\n",
    "\n",
    "You can use the following code for this step (save the below code into a file called `kafka_wordcount.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: kafka_wordcount.py <zk> <topic>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    sc = SparkContext(appName=\"PythonStreamingKafkaWordCount\")\n",
    "    ssc = StreamingContext(sc, 10)\n",
    "\n",
    "    zkQuorum, topic = sys.argv[1:]\n",
    "    kvs = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})\n",
    "    lines = kvs.map(lambda x: x[1])\n",
    "    counts = lines.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "    counts.pprint()\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run the Wordcount Application\n",
    "\n",
    "Now that we have the above code saved in a `kafka_wordcount.py` file, the next step is to use `spark-submit` to utilize Spark to process the data.\n",
    "\n",
    "Submit the application using `spark-submit` with dynamic allocation disabled and specifying your ZooKeeper server and topic. To run locally, you must specify at least two worker threads: one to receive and one to process data.\n",
    "\n",
    "Run the following command (ensure you put your correct path for `SPARK_HOME` and your corresponding `spark-example` jar file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark submit command\n",
    "spark-submit --master local --deploy-mode client --conf \"spark.dynamicAllocation.enabled=false\" --jars SPARK_HOME/jars/spark-examples_2.12-3.2.0.jar kafka_wordcount.py zookeeper_server:2181 wordcounttopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important arguments:__\n",
    "- `master`:\n",
    "    -   Specifies the manager to use for the cluster.  In this example, we use local, but Spark also supports Mesos, Kubernetes, stand-alone and Yarn.\n",
    "- `deploy-mode`:\n",
    "    -   Specifies whether to deploy the driver program on the worker nodes (cluster mode) or on the local machine (client mode)\n",
    "- `conf`\n",
    "    -   Specifies the configuration parameters to use.  These configurations are numerous and are used to specify application configurations, shuffle parameters and other runtime configurations.\n",
    "- `jars`:\n",
    "    -    Path to the bundled jar file containing the application code and all required dependencies.\n",
    "- `kafka_wordcount.py`:\n",
    "    -    The Python file to run\n",
    "- `zookeeper_server`\n",
    "    -    Specifies the Zookeeper server details and port number\n",
    "\n",
    "\n",
    "For a detailed description of all `spark-submit` arguments, run the following command in an open terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark-submit detailed options description\n",
    "bash /bin/spark-submit --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming all goes well, you should see the application running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Open a Kafka producer\n",
    "\n",
    "Next, we need a Kafka producer to provide data to Spark.  To create one, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka producer for the topic Wordcount \n",
    "kafka-console-producer --broker-list kafka_broker:9092 --topic wordcounttopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can enter data in the terminal, which will then be picked up by Spark, and the words will be counted in real-time.\n",
    "\n",
    "To test this, type the following data in the Kafka producer window:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Customer ID`\n",
    "- `Product ID`\n",
    "- `Timestamp`\n",
    "- `Address`\n",
    "- `Price`\n",
    "- `Phone Number`\n",
    "- `Email`\n",
    "- `Gender`\n",
    "- `First Name`\n",
    "- `Last Name`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how fast you type, in the Spark Streaming application window, you'll see output similar to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Time: 2016-01-06 14:18:00`\n",
    "\n",
    "- `(u'Customer', 1)`\n",
    "- `(u'Product', 1)`\n",
    "- `(u'ID', 2)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the hands-on tutorial and the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "- Kafka and Spark are designed to integrate smoothly together without major configuration or coding effort.\n",
    "- Kafka can connect to flat files, Twitter feeds and other types of data.\n",
    "- Kafka can be used to produce a constant stream of data that Spark Streaming can ingest and manipulate.\n",
    "- Kafka and Spark Streaming together can handle both batch and real-time data\n",
    "- It's possible to implement different transformations on the data in real-time such as Count and Sort.\n",
    "- To run a more complex application, we should use `spark-submit` and pass it a file containing our PySpark code and run this command from the terminal.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6971ff02672853a145ab8a619e17e4c2b989e1ba4684228133b86b474ce57f92"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
