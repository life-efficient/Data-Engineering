{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "australian-indie",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "> __Spark is a unified engine for large-scale data processing on computer clusters__\n",
    "\n",
    "Originally written in [__Scala__](https://www.scala-lang.org/) programming language, open source project available on [GitHub](https://github.com/apache/spark).\n",
    "\n",
    "## Supported language frontends\n",
    "\n",
    "Official APIs are provided for different languages:\n",
    "- [PySpark](https://spark.apache.org/docs/latest/api/python/) - as the name suggests Python frontend for Spark\n",
    "- [Java API](https://sparkjava.com/) - as Scala is based off JVM and Java language with high interoperability before both languages\n",
    "- [SparkR](https://spark.apache.org/docs/latest/sparkr.html) - [R langauge](https://www.r-project.org/) front-end for statistical oriented code\n",
    "\n",
    "> __We will use PySpark in order to interact with Spark engine__\n",
    "\n",
    "## High level libraries\n",
    "\n",
    "High level libraries are provided on top top of `Spark`, namely:\n",
    "- [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) - Query language for data processing\n",
    "- [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) - Machine Learning on Spark computing engine\n",
    "- [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) - graph related operations\n",
    "- [Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) - streaming related operations\n",
    "\n",
    "> __In this notebook we will focus on core Spark functionalities__\n",
    "\n",
    "> Other functionalities can be used on the same engine, __please refer to documentation if you need specific part in your workflow__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7207113",
   "metadata": {},
   "source": [
    "# Cluster mode overview\n",
    "\n",
    "Before we dive in let's see what the engine consists of in more detail and how can we choose one:\n",
    "\n",
    "![](./images/pyspark-driver-executor.png)\n",
    "\n",
    "## Application\n",
    "\n",
    "> __User program built with Spark__\n",
    "\n",
    "It usually runs on __cluster__ and consists of:\n",
    "- Cluster manager\n",
    "- Driver program\n",
    "- Executor\n",
    "\n",
    "More on them below:\n",
    "\n",
    "## Cluster Manager\n",
    "\n",
    "> __Cluster manager is a program handling resources to our application(s)__\n",
    "\n",
    "Cluster manager is responsible for:\n",
    "- Handling requests from a driver for resources\n",
    "\n",
    "There are a few available options, most important of which are:\n",
    "- Local - run everything on a single machine (__non distributed!__)\n",
    "- [Standalone](https://spark.apache.org/docs/latest/spark-standalone.html) - PySpark \"default\" cluster manager\n",
    "- [Apache Mesos](https://spark.apache.org/docs/latest/running-on-mesos.html) - Apache Spark \"modern\" approach, useful for __more generic workloads__\n",
    "- [Hadoop YARN](https://spark.apache.org/docs/latest/running-on-yarn.html) - Apache Spark \"older\" approach, specific for Hadoop oriented operations (e.g. map-reduce)\n",
    "- [Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html) - container first auto-scalable workloads\n",
    "\n",
    "### How to choose cluster manager?\n",
    "\n",
    "#### Local\n",
    "\n",
    "> Use it when you are developing and testing your app\n",
    "\n",
    "As it is the simplest one, we can verify everything works correctly using a single machine\n",
    "\n",
    "#### Standalone\n",
    "\n",
    "> __Small clusters WORKING ONLY WITH PYSPARK APPLICATION__\n",
    "\n",
    "This one, while it doesn't require additional software has the falling drawbacks:\n",
    "- We cannot run other workload on it (e.g. monitoring)\n",
    "- PySpark first\n",
    "- __Runs main and child processes of PySpark on each node__ hence it has an additional overhead\n",
    "\n",
    "#### Mesos\n",
    "\n",
    "> __Larger/production clusters with GENERAL capabilities__\n",
    "\n",
    "- Better for new projects\n",
    "- More generic than YARN\n",
    "- Good option for non-containerized \n",
    "\n",
    "#### YARN (Hadoop 2.0)\n",
    "\n",
    "> __Larger/production clusters with GENERAL capabilities BETTER AT RUNNING HADOOP SPECIFIC OPERATIONS__\n",
    "\n",
    "Other than that quite similar to Mesos\n",
    "\n",
    "![](./images/spark-standalone-hadoop.png)\n",
    "\n",
    "\n",
    "#### Kubernetes\n",
    "\n",
    "> __Workloads which can autoscale (create more/less instances based on workload) and containerized__\n",
    "\n",
    "This one has a lot of benefits and becomes a go-to for the following reasons:\n",
    "- We can containerize most of the applications\n",
    "- Because of that our deployment is streamlined and less error-prone (different OS different behaviour)\n",
    "- __Autoscaling__ - create more node workers if needed\n",
    "- __Available as service for many clouds__ ([Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks/), [Google Cloud Kubernetes Engine](https://www.google.com/search?client=firefox-b-d&q=Google+Cloud+Kubernetes+engine) or [Microsoft's Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/))\n",
    "\n",
    "This approach scales well across:\n",
    "- different regions if needed\n",
    "- different cloud providers if needed\n",
    "- for smaller teams (for which handling Kubernetes cluster is too costly) via out-of-the-box cloud solutions\n",
    "\n",
    "#### Nomad\n",
    "\n",
    "> __Workloads mixing containerized and non-containerized workloads across large amount of clusters__\n",
    "\n",
    "Similiar to `k8s` but:\n",
    "- No autoscaling out of the box (needs additional software for that)\n",
    "- Smaller community support\n",
    "- __Easier to use than Kubernetes__\n",
    "- __Less popular than Kubernetes__\n",
    "\n",
    "\n",
    "## Driver program\n",
    "\n",
    "> __Main program which orchestrates jobs in our cluster__\n",
    "\n",
    "It's job is to:\n",
    "- Acquires __executors__ on worker nodes by requesting them from cluster manager via __SparkContext__\n",
    "- __Sends code to the executors__, one of:\n",
    "    - Python files (in case of PySpark)\n",
    "    - JAR files for Scala/Java code\n",
    "- __Sends tasks to the executors__, which are __single unit of work send to a single executor__\n",
    "\n",
    "> __SENDING CODE IS DONE VIA `spark-submit` script we will later see how to use!__\n",
    "\n",
    "\n",
    "## Executor\n",
    "\n",
    "> __Processes which run computations and store data__\n",
    "\n",
    "Data can be stored in a few different ways which we will later talk about (see `Data Locality` below).\n",
    "\n",
    "Things to note:\n",
    "- __Each application has a single executor on the node__\n",
    "- __There might be multiple executors on a single node__\n",
    "- Due to above applications are isolated (each is run in a separate JVM machine)\n",
    "- __DATA CANNOT BE EASILY SHARED BETWEEN SPARK APPLICATIONS__ (we need to save the data in some widely available storage like Kubernetes volumes for other apps to use)\n",
    "\n",
    "## Useful things to note\n",
    "\n",
    "> __See [glossary](https://spark.apache.org/docs/latest/cluster-overview.html) for a quick reminder of all of the concepts__\n",
    "\n",
    "- __Job is a set of parallel tasks__ distributed across the cluster, for example `collect` across nodes\n",
    "- __Driver should be close to workers__ (or most of them) as it orchestrates the whole workload (best when in the same local network if possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb63be",
   "metadata": {},
   "source": [
    "# HDFS (Hadoop Distributed FileSystem)\n",
    "\n",
    "As different cluster managers can handle data differently, hence data could be (theoretically):\n",
    "- shared across cluster\n",
    "- shared across parts of the cluster\n",
    "- kept local for each node\n",
    "- kept local and pulled/moved around\n",
    "\n",
    "> __This would affect computation speeds tremendously!__ \n",
    "\n",
    "Apache has an answer to that: __HDFS__\n",
    "\n",
    "![](./images/hdfsarchitecture.png)\n",
    "\n",
    "## What is it?\n",
    "\n",
    "> __Distributed filesystem where parts of data are on different nodes in the cluster__\n",
    "\n",
    "This approach (and architecture of HDFS) comes with a few key points to note:\n",
    "1. __Node failure resistance__:\n",
    "    - each node has part of data (and some data replicas)\n",
    "    - when one node goes offline needed data replicas can be recreated (we will see the `NameNode` shortly)\n",
    "    - Hadoop 3.0 introduced parity bits in order to use `50%` data overhead for fault tolerance instead of default `200%` (`3` replicas)\n",
    "2. __HIGH THROUGHPUT__ (latency does not matter):\n",
    "    - system is designed to retrieve __LARGE BATCHES OF DATA__ as quick as possible\n",
    "    - __NOT DESIGNED FOR ACCESS BY SINGLE USER__\n",
    "3. __LARGE DATASETS__ (in the order of petabytes)\n",
    "4. __WRITE-ONCE-READ-MANY__:\n",
    "    - data is created only once and distributed across the cluster\n",
    "    - __it should not be changed afterwards__\n",
    "    - __it is read-only__ (hence can be done in parallel without data races)\n",
    "5. __MOVES COMPUTATION CLOSER TO DATA__:\n",
    "    - data is not moved around (as it usually requires large transfers across the web)\n",
    "    - __source code is moved instead__ and run on each node separately\n",
    "6. __Created for large files__ (one large file better than million of small ones)\n",
    "7. Portability across different system\n",
    "\n",
    "\n",
    "## NameNode(s)\n",
    "\n",
    "> __Similar to Driver seen before, manages the distributed filesystem__\n",
    "\n",
    "- Stores information about block locations\n",
    "- Contains log of all changes to the system (additions, deletions etc.)\n",
    "- Opens, renames and edit files in worker nodes\n",
    "- In case of failure (__based on `DataNode` hearbeat__) new replicas are send to the rest of the filesystem\n",
    "\n",
    "> Since Hadoop 2.0 there is an additional NameNode __as previously it was a single point of failure for the system__\n",
    "\n",
    "## DataNode\n",
    "\n",
    "> __Commodity hardware containing BLOCK OF OUR DATA__\n",
    "\n",
    "When we create data in HDFS it:\n",
    "- Gets splitted in blocks\n",
    "- Blocks are moved by `NameNode` to `DataNode`s (distributed)\n",
    "- Any operations carried out by `NameNode` update appropriate `DataNode`s\n",
    "\n",
    "What is done by Spark (and Hadoop) is:\n",
    "- This is where computations are carried on the data\n",
    "- This is where results of them are saved\n",
    "\n",
    "## Blocks\n",
    "\n",
    "> __Block is a single file split in a bit-wise fashion__\n",
    "\n",
    "![](./images/block-replication.jpg)\n",
    "\n",
    "By default, each block is of size `128Mb`, if a file is smaller __the whole block IS NOT occupied!__\n",
    "\n",
    "## Reading and writing files\n",
    "\n",
    "> As HDFS is a filesystem it supports reading and writing files __directly__ (although we will not do this in our lesson)\n",
    "\n",
    "Following steps are done for `write`:\n",
    "- Client (let's say us) ask `NameNode` to store a file\n",
    "- `NameNode` replies with `DataNode`s adresses __and addresses of their replicas__\n",
    "- __For each block__:\n",
    "    - It is send to the first `DataNode`\n",
    "    - Another `DataNode` is chosen __in the same rack__ (computers connected via the same switch)\n",
    "    - Data is sent to it __as a replica__\n",
    "    - Another `DataNode` is chosen __in different rack__ and data is sent to it\n",
    "- After all blocks have been transfered correctly, client receives a `0` return code\n",
    "\n",
    "\n",
    "For `read` operation:\n",
    "- Asking about metadata (same as during `write`)\n",
    "- Direct connection with `DataNode`s\n",
    "- Data flows directly from `DataNode`s __in parallel__\n",
    "- After receiving all of them they are formed back into a single file\n",
    "\n",
    "Now, that we have an idea of the filesystem used by Spark, let's see a common calculation approach..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434afb2",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "> __Processing layer over distributed filesystem designed for processing large volumes of data in parallel by dividing work into a set of independent tasks__\n",
    "\n",
    "The idea works as follows:\n",
    "- User submits __job__ (usually large amount of work) which consists of:\n",
    "    - Execution of __Mapper__\n",
    "    - Execution of __Reducer__\n",
    "- __Job__ is splitted into tasks (done in parallel by worker nodes)\n",
    "- These are sent to child processes on cluster\n",
    "- __Individual Mapper and Reducer executions are done on each node__\n",
    "- Each task returns an output which is latter aggregated to give final result\n",
    "\n",
    "> __MapReduce operates on lists!__\n",
    "\n",
    "This means:\n",
    "- Input to our functions are lists\n",
    "- Procedures outputs lists\n",
    "- __Functional programming approach__ (data is unmutable)\n",
    "\n",
    "## Task Attempt\n",
    "\n",
    "> Each node can attempt to perform a task (Task In Progress a.k.a. TIP status) __but may fail due to various reasons__\n",
    "\n",
    "If a node fails:\n",
    "- Hadoop reschedules the task to other node\n",
    "- It can be done multiple times (__up to `4` by default__)\n",
    "- After that program fails\n",
    "\n",
    "## High level flow\n",
    "\n",
    "![](./images/map_reduce_counting.png)\n",
    "\n",
    "Let's see how we obtain results step by step by extending the diagram above:\n",
    "1. Our input data (usually saved in HDFS), in this case text\n",
    "2. `InputFormat` defines:\n",
    "    - __How to split data__\n",
    "    - __How to read them__\n",
    "    - __Creates `InputSplit`s__\n",
    "3. __`InputSplit`s represent data processed by each `Mapper`__:\n",
    "    - One `map` task for each split\n",
    "    - `InputSplit` is divided into separate records\n",
    "    - And these records are processed by `map` operation\n",
    "4. __`RecordReader`__ communicates with `InputSplit` to:\n",
    "    - Transform the split into readable format for mapper (`(key, value)` pairs)\n",
    "5. __`Mapper`__ - processes `(key, value)` pair from `RecordeReader` and:\n",
    "    - Generates new `(key, value)` pair\n",
    "    - __Does it by our specified logic__ (in this case counting word occurences)\n",
    "    - Outputs values to disk creating __temporary results__ (__THESE ARE NOT SAVED TO HDFS!__)\n",
    "6. __`Combiner`__ (a.k.a. `mini-reducer`) - takes temporary values and:\n",
    "    - Combines them into larger batches\n",
    "    - This is done in order to minimize data transfers over the network\n",
    "7. __`Partitioner`__ (__USED ONLY FOR MULTIPLE `Reducer`s__):\n",
    "    - Takes output from `combiner`\n",
    "    - __`key` is used to make a single partition__ (in our case specific word)\n",
    "    - __Records having the same `(key, value)`s are called a partition__\n",
    "    - __GUARANTEES APPROXIMATELY THE SAME LOAD FOR EACH `Reducer`__\n",
    "8. __Shuffling and Sorting__ - data set via network to Reducer notes:\n",
    "    - __Each `Reducer` might get multiple partitions__\n",
    "    - Each partition is sorted so they are __a consecutive block of data__\n",
    "9. __`Reducer`__ - takes combined values from previous step and:\n",
    "    - Runs __user defined reduction operation__ on each temporary `(key, value)` pair \n",
    "    - In our case it counts how many of the same records are there\n",
    "    - __Stores output on HDFS__ via `RecordWriter`\n",
    "    - We can modify it (e.g. in Java) via specifying custom `OutputFormat` ([documentation](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/OutputFormat.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea05b3",
   "metadata": {},
   "source": [
    "## Map Reduce FAQ\n",
    "\n",
    "There might be a few misconceptions, so let's clear them out:\n",
    "\n",
    "> Why shuffle happens?\n",
    "\n",
    "It happens as chunks of data are moved across the network. They might:\n",
    "- come at different times\n",
    "- __from any node in the HDFS__\n",
    "\n",
    "Hence they are unorganized on disk __and that's why we have to sort them afterwards__\n",
    "\n",
    "> How many mappings are run on one node?\n",
    "\n",
    "Usually around `100` parallel tasks __per node__ are run. For lighter tasks, up to `300` is reasonable\n",
    "\n",
    "> Why sorting twice?\n",
    "\n",
    "__This is done only for multiple `Reducers`__ in order to:\n",
    "- Make the network congestion smaller (because single partition will land on a single `Reducer`)\n",
    "- There might be multiple \"same\" partitions (based on `(key, value)`) from different mappers\n",
    "- Multiple partitions might be processed by one node\n",
    "\n",
    "Let's look at the last example:\n",
    "1. There are `3` `A` and `3` `B` partitions in total in HDFS network\n",
    "2. Each partition lies on different mapper node\n",
    "3. Intermediate results are send to `Reducers`\n",
    "\n",
    "We might obtain the following (__already sorted by mappers!__) data scheme: `ABABABAB`. This means we have to sort them once again.\n",
    "\n",
    "> Can `Reducer` run when some of the `mappers` did not finish?\n",
    "\n",
    "__No__ as it might mean \"reduce\" operation would need to be recalculated. This is done only after \"aggregating\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62398525",
   "metadata": {},
   "source": [
    "# Spark vs Hadoop\n",
    "\n",
    "Hadoop itself was mentioned a few times, what is it and what's the difference between it and Spark?\n",
    "\n",
    "> __Hadoop consists of HDFS, MapReduce computational layer and YARN (Hadoop cluster manager)__\n",
    "\n",
    "Written in `Java` and released in `2006`, provides multiple front-end languages to interact as well. When compared to `Spark`:\n",
    "\n",
    "- It is a batch-processing large-scale data-efficient processing framework\n",
    "- __DOES NOT PROVIDE REAL-TIME CAPABILITIES FOR CALCULATIONS__ because:\n",
    "    - Writing to disk all the time is too slow\n",
    "    - Solved by `Spark`\n",
    "- __`Spark` DOES IT'S COMPUTATIONS IN-MEMORY WHEREVER POSSIBLE__:\n",
    "    - Not writing to disk intermediate results from nodes (__or at least does not for a part of data which fits in RAM__)\n",
    "    - Due to above `Spark` is about `100` times faster\n",
    "- __Spark can use multiple cluster managers__\n",
    "- __Spark is more of a \"high-level\" tool__ which uses various concepts from `Hadoop` and applies abstraciton layer over it\n",
    "- __It does not provide specific functionalities__ like Spark (e.g. `MLLib`)\n",
    "- __It is a part of Apache Spark__ (e.g. using `HDFS` and `YARN` as cluster manager)\n",
    "\n",
    "Essentially we have all of the pieces of Hadoop in place (at least in theory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34ce43",
   "metadata": {},
   "source": [
    "# Data Locality\n",
    "\n",
    "> __Data Locality is the process of moving COMPUTATIONS closer to DATA__ (so they are run locally a.k.a. \"per-node\")\n",
    "\n",
    "In general, if `data` and `operations` reside close to each other the whole computation will be fast.\n",
    "In other cases, these might be slower, hence __computation has to be moved towards data__.\n",
    "\n",
    "There are a few possibilities when it comes to data locality in Spark (__ordered by best to worst__):\n",
    "1. `PROCESS_LOCAL` - __code is in the same `JVM` as data__ \n",
    "2. `NODE_LOCAL` - __data on the same node__, for example:\n",
    "    - HDFS on the same node\n",
    "    - Another executor on the same node\n",
    "    - __Data has to travel between processes__\n",
    "3. `NO_PREF` - data has no preference where it is located because:\n",
    "    - It does not matter for computation\n",
    "    - __Example:__ shared volumes in `k8s`\n",
    "4. `RACK_LOCAL` - data on the same rack of servers, __data has to be send through a single switch in the network__\n",
    "5. `ANY` - data is elsewhere on the network, __not in the same rack__\n",
    "\n",
    "__When `Spark` does scheduling for the computations it does it w.r.t. data locality__ which means:\n",
    "- `Spark` checks whether best node to process data is available\n",
    "- If not `Spark` waits for the busy `CPU` with best data locality to finish it's computation __but only for a short while__ \n",
    "- If it does not finish in a predefined `timeout` __spark moves data to next free `CPU`__\n",
    "\n",
    "> __One can control data locality via `spark.locality` setting we will later see__\n",
    "\n",
    "> __YOU SHOULD INCREASE TIMEOUT IF YOU SEE POOR DATA LOCALITY WITH DEFAULT SETTINGS!__\n",
    "\n",
    "> __Timeouts should be traced to how long your jobs run on the cluster__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee0c70",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "## Assessment\n",
    "\n",
    "- Check out [Apache Myriad Project](https://incubator.apache.org/projects/myriad.html) - what might be it's benefits for cluster management in PySpark?\n",
    "- What is secondary `NameNode` and what is it's purpose in Hadoop's FileSystem?\n",
    "- Check out how to work with Hadoop's FileSystem via command line using this series of tutorials ([1](https://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/), [2](https://data-flair.training/blogs/hadoop-hdfs-commands/) and [3](https://data-flair.training/blogs/hdfs-hadoop-commands/))\n",
    "- Why `Partitioner` __IS NOT__ used in Hadoop MapReduce processing layer with single `Reducer`?\n",
    "\n",
    "\n",
    "## Non-assessment\n",
    "\n",
    "- What is SIMR approach included in the second graphic in this notebook? Check out [this article](https://databricks.com/blog/2014/01/21/spark-and-hadoop.html)\n",
    "- What is RAID and how does Erasure Coding work in Hadoop? Check [this tutorial](https://data-flair.training/blogs/hadoop-hdfs-erasure-coding/)\n",
    "- What are `BackupNode`s and `CheckpointNode`s in HDFS?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AiCore] *",
   "language": "python",
   "name": "conda-env-.conda-AiCore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
