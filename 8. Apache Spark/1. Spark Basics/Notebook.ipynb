{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159173ce",
   "metadata": {},
   "source": [
    "# Spark basics\n",
    "\n",
    "One of the most popular ways to use Spark is with the Python library `pyspark`.\n",
    "\n",
    "# Installation\n",
    "\n",
    "PySpark has many dependencies, not only with other Python packages, but also with other modules that are not easily installed using the convenient `pip install` command. While you can install pyspark using `pip install pyspark` this is probably not going to be enough. Therefore, we recommend you to follow the next steps:\n",
    "\n",
    "1. Visit [PySpark download page](https://spark.apache.org/downloads.html) and:\n",
    "- Choose latest release\n",
    "- Download package locally\n",
    "\n",
    "2. Create a folder (for example `spark`)  in a directory that you know will be safe. `~/` is usually a good option. \n",
    "3. Extract the files from the downloaded file into the created folder. At the time of writing, the last version was Spark 3.1.2, so, in that case, your directory will look like this (in case you are using the same examples):\n",
    "```\n",
    "~/\n",
    "│\n",
    "├── spark/\n",
    "│   └── spark-3.1.2-bin-hadoop3.2  <--- SPARK_HOME\n",
    "│         ├── bin\n",
    "│         ├── conf\n",
    "│         ├── data\n",
    "... \n",
    "```\n",
    "4. It is important you set the directory as SPARK_HOME, otherwise, PySpark won't know where to find the corresponding commands. To do so, simply set it as a environment variable copying the following command in your `~/.bashrc` file:\n",
    "\n",
    "`export SPARK_HOME=<path to your home directory>/spark/spark-3.1.2-bin-hadoop3.2`\n",
    "\n",
    "_Note: The command above depends on where you extracted the files you downloaded and the version_\n",
    "\n",
    "> Don't skip this step. Having an incorrectly set `SPARK_HOME` environment variable is the cause of many common issues with Spark\n",
    "\n",
    "5. Save your `~/.bashrc`. You should be able to use PySpark now! If not, try restarting vscode, then try restarting your computer if that doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108de564",
   "metadata": {},
   "source": [
    "6. To check if the installation was successful, you can install findspark (`pip install findspark`) and run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0262f36c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in ~/tools/spark-3.2.1-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3z/29w5rr9d0k3_p863hm40sdnc0000gn/T/ipykernel_5102/1796740182.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Unable to find py4j in ~/tools/spark-3.2.1-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332c901",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "  <summary> <font size=+1> For Windows Users </font> </summary>\n",
    "  \n",
    "  Depending on your environment, the last steps might not work. In that case, you have to set the environment variable manually. Look at the following gif to know how to it\n",
    "\n",
    "  <p align=center><img src=images/Spark_home.gif></p>\n",
    "\n",
    "  If this still doesn't work\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d114af81",
   "metadata": {},
   "source": [
    "## findspark\n",
    "\n",
    "The Spark functionalities might not be discoverable within a script or a notebook, so you can use `findspark` which will set the script or notebook to keep using Spark interactively. Remember that:\n",
    "\n",
    "1. Inside the script you are going to define the instructions\n",
    "2. Those instructions will be orchestrated amongst the executors using Spark\n",
    "3. PySpark will be the API that helps you write in Python the instructions. Then, those instructions will be translated, so Spark actually understands it\n",
    "\n",
    "Thus, you will create the script using PySpark, and then, you will send that script to Spark, usually using spark-submit, which we will see later in this notebook.\n",
    "\n",
    "`findspark` will be useful when you are developing your application, to check if spark will respond the way you expect while you are writing your code.\n",
    "\n",
    "- Run `findspark.init()` (which will set up necessary environment variables so `pyspark` can connect to JVM)\n",
    "- You can also tun `findspark.find()` to see the directory where `SPARK_HOME` has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e915bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ca5fa",
   "metadata": {},
   "source": [
    "## Spark config\n",
    "\n",
    "Given all of the steps above, we can set up Spark's distributed processing engine using:\n",
    "- A programmatic interface (`pyspark` in our case) - usable for application specific tasks and varying configuration\n",
    "- Command line - usable for `spark-submit` and __overriding default values__\n",
    "- Config file - usable as a base config and __when we submit job to the cluster__\n",
    "- Global config file\n",
    "\n",
    "> Above is also a priority list and the config for each overides the config from the ones below it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254d4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1g\n",
      "spark.master=local[8]\n",
      "spark.app.name=TestApp\n",
      "spark.eventLog.enabled=False\n",
      "spark.executorEnv.VAR3=value3\n",
      "spark.executorEnv.VAR4=value4\n",
      "spark.executor.memory=1g\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import pyspark\n",
    "\n",
    "cfg = (\n",
    "    pyspark.SparkConf()\n",
    "    # Setting where master node is located [cores for multiprocessing]\n",
    "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\n",
    "    # Setting application name\n",
    "    .setAppName(\"TestApp\")\n",
    "    # Setting config value via string\n",
    "    .set(\"spark.eventLog.enabled\", False)\n",
    "    # Setting environment variables for executors to use\n",
    "    .setExecutorEnv(pairs=[(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
    "    # Setting memory if this setting was not set previously\n",
    "    .setIfMissing(\"spark.executor.memory\", \"1g\")\n",
    ")\n",
    "\n",
    "# Getting a single variable\n",
    "print(cfg.get(\"spark.executor.memory\"))\n",
    "# Listing all of them in string readable format\n",
    "print(cfg.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f8165",
   "metadata": {},
   "source": [
    "# Sessions\n",
    "\n",
    "> PySpark's session object provides a unified connection to our Spark cluster.\n",
    "\n",
    "There are a few ways to set up the Spark session:\n",
    "- directly through named/unnamed arguments\n",
    "- using `SparkConf` object (which we created and will use)\n",
    "- Providing `SparkContext` with settings (this is deprecated, avoid doing this)\n",
    "\n",
    "The Spark session is used to:\n",
    "- create `DataFrame`s (the main object containing data within cluster)\n",
    "- broadcast variables to machines within the cluster\n",
    "- Run operations across HDFS enabled systems\n",
    "\n",
    "Spark and `pyspark` provide a few objects that can be used to interact with the Spark engine:\n",
    "- `pyspark.SparkContext`\n",
    "- `org.apache.spark.sql.SQLContext` (Only for Scala)\n",
    "- `org.apache.spark.sql.hive.HiveContext` (Only Scala)\n",
    "- `pyspark.sql.SparkContext`\n",
    "\n",
    "What are these different options and why do they exist?\n",
    "\n",
    "### SparkContext\n",
    "\n",
    "> `SparkContext` is the object used by any driver to communicate with the cluster manager, execute and coordinate jobs\n",
    "\n",
    "This object is always used under the hood, if not directly, to interact with the cluster. Direct use of the Spark context is deprecated and should be avoided.\n",
    "\n",
    "### SQLContext\n",
    "\n",
    "Previously, you had to provide `SparkContext` to this object in order to interact with SQL-like capabilities (e.g. creating a `DataFrame`) using the `SparkSQL` library\n",
    "\n",
    "### HiveContext\n",
    "\n",
    "> __Extension of SQLContext providing gateway to Hive__\n",
    "\n",
    "Hive is similar in structure to SQL but provides capabilities for data warehousing and is better suited for analyzing large scale data\n",
    "\n",
    "## SparkSession\n",
    "\n",
    "In Spark `v2.0` one object to rule them all was introduced. That was `spark.SparkSession`. It wraps functionalities of all of the contexts introduced above (SparkContext, SQLContext, HiveContext) into one API.\n",
    "\n",
    "In `pyspark` one can use it via `spark.sql.SparkSession`. \n",
    "\n",
    "The the `builder` attribute has methods to obtain the appropriate `SparkSession`.\n",
    "\n",
    "It's config method can be used to firstly set the config.\n",
    "\n",
    "The `getOrCreate` method does the following:\n",
    "- If no global `Session` exists create a new one with specified config\n",
    "- If global `Session` exists:\n",
    "    - Get an instance of it\n",
    "    - Apply the new configuration to it\n",
    "\n",
    "This approach is safe as using multiple context is a bad practice (although possible)\n",
    "\n",
    "\n",
    "This SparkSession can be used just like the other context objects were historically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d90bd58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T14:34:35.447266Z",
     "start_time": "2021-07-10T14:34:35.246044Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = pyspark.sql.SparkSession.builder.config(conf=cfg).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa53f8b",
   "metadata": {},
   "source": [
    "# Data Structures\n",
    "\n",
    "Before diving in we need to talk about `3` available data structures in `spark`:\n",
    "- `RDD` - Resilient Distributed Dataset - fault-tolerant collection of elements that can be operated on in parallel\n",
    "- `DataFrame` -  dataset organised into named columns. Conceptually equivalent to a table in a relational database or a dataframe in R/Python, but with richer optimisations under the hood.\n",
    "- `Dataset` - distributed collection of data. Provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine\n",
    "\n",
    "![](./images/rdd_df_dataset_history.png)\n",
    "\n",
    "# RDD & Core Spark API\n",
    "\n",
    "> __Core and basic of Spark applications with \"low-level\" operations__\n",
    "\n",
    "> __Fault-tolerant collection of elements that can be operated on in parallel.__\n",
    "\n",
    "This structure provides strong typing (via `JVM` objects) and can be constructued in two ways:\n",
    "- __parallelizing existing collection__ (e.g. Python's `list`)\n",
    "- __referencing dataset in external storage__ (anything compatible with Hadoop's InputFormat like HDFS, HBase, Amazon S3, text files etc.)\n",
    "\n",
    "Let's see these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bdde61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDistributedData = session.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c7744",
   "metadata": {},
   "source": [
    "__Things to note for files__:\n",
    "- __Each file has to be in the same path on each worker node!__ (in our case we are running locally hence this is fine)\n",
    "- All file-based methods operate on:\n",
    "    - directories - `textFile(\"/my/directory\")`\n",
    "    - wildcards - `textFile(\"/my/directory/*.txt\")`\n",
    "    - compressed files - `textFile(\"/my/directory/*.gz\")`\n",
    "- We can change number of partitions created for this file\n",
    "- See API [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html#pyspark.SparkContext.textFile)\n",
    "\n",
    "> __Other ways to create RDD from file can be seen in [Spark Context API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#spark-context-apis), e.g. a way to create it from `pickle`__\n",
    "\n",
    "## Lazy Evaulation\n",
    "\n",
    "> Created RDDs __ARE NOT FILES__, they are merely a description of operation that __has to be run at some point__\n",
    "\n",
    "What we did above means:\n",
    "- Parallelize `list` operation\n",
    "- Read from text file `lorem.txt` (__but the read wasn't performed!__)\n",
    "\n",
    "> All of the operations will be run when we __request an ACTION__\n",
    "\n",
    "Actions may include:\n",
    "- return number of lines in file (whole map-reduce went through)\n",
    "- sum the list and return the result\n",
    "\n",
    "## Persist\n",
    "\n",
    "> Persisting is used in order to speed-up computations (saving intermediate results in memory)\n",
    "\n",
    "If we run the line below it means:\n",
    "\n",
    "> Read data file and cache read contents in the memory (if possible)\n",
    "\n",
    "> __If we run \"action\" on the file it will use the cached data (faster) rather than loading data from disk once again!__\n",
    "\n",
    "Rule of thumb: \n",
    "\n",
    "> Use cache when the lineage (operations to run on certain RDD) of your RDD branches out or when an RDD is used multiple times like in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aff9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the operations return self\n",
    "# This allows us to chain operations (we will see it in the next cell)\n",
    "\n",
    "rddDistributedFile = rddDistributedFile.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614526c4",
   "metadata": {},
   "source": [
    "> __`.cache()` is the same as `.persist()` with `StorageLevel.MEMORY_ONLY`__\n",
    "\n",
    "There are few other options to store the data:\n",
    "- `MEMORY_ONLY` - keep everything we can in memory otherwise do not cache and compute results\n",
    "- `MEMORY_AND_DISK` - keep everything we can in memory otherwise serialize to disk (__encouraged for long running computations we would like to cache__)\n",
    "- `DISK_ONLY` - cache everything on disk, nothing in memory (__discouraged__)\n",
    "- `MEMORY_ONLY_2` - same as `MEMORY_ONLY` but replicates cache on two cluster nodes for improved fault tolerance (`DISK_ONLY_2` is also available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "666b62a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, False, False, False, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.StorageLevel.DISK_ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035bfe3",
   "metadata": {},
   "source": [
    "## MapReduce operations\n",
    "\n",
    "> Given parallelized data structure we can run map-reduce operations on it\n",
    "\n",
    "All of them can be seen [in the documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis), a few interesting ones:\n",
    "- [`rdd.checkpoint()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.checkpoint.html#pyspark.RDD.checkpoint) - will be saved in checkpoint directory and all the operations creating it __are discarded__ (action)\n",
    "- [`rdd.collect()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.collect.html#pyspark.RDD.collect) - __return the structure__ (collect it after operations) (action)\n",
    "- [`rdd.count()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.count.html#pyspark.RDD.count) - count elements in the structure (action)\n",
    "- [`rdd.countByKey()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByKey.html#pyspark.RDD.countByKey) - count number of elements for each `key` in `(key, value)` pairs (similar to what the graphic before did)\n",
    "- [`rdd.countByValue()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByValue.html#pyspark.RDD.countByValue) - count __how many unique values__ are in this structure (returned as `(value, count)` dictionary)\n",
    "\n",
    "__And the essential ones we will use are:__\n",
    "- [`rdd.map(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html#pyspark.RDD.map) - apply function __to each element in the collection__\n",
    "- [`rdd.filter(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html#pyspark.RDD.filter) - __choose values which fulfill `f` function__\n",
    "- [`rdd.flatMap(f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.flatMap.html#pyspark.RDD.flatMap) - __apply function to each element and `flatten` the list if necessary__\n",
    "- [`rdd.fold(neutralValue, f)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.fold.html#pyspark.RDD.fold) - __given associative function (like `add`) takes every 2 elements together and returns the result__\n",
    "- [`rdd.sortBy(keyfunction)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html#pyspark.RDD.sortBy) - sort by specific function which returns some value from the `(key, value)` pair\n",
    "\n",
    "> __PLEASE REFER TO DOCUMENTATION WHEN LOOKING FOR AN OPERATOR! MANY OF THEM ARE ALREADY IMPLEMENTED!__\n",
    "\n",
    "> __TAKE TIME TO COME UP WITH THE OPERATORS NEEDED! EACH OPERATION SAVED MIGHT IMPROVE RUNTIME TREMENDOUSLY!__\n",
    "\n",
    "Let's see an example chaining on data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3051cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc is standard name for sparkContext\n",
    "# it will be easier to use from now on\n",
    "\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c63be368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "data = list(range(10,-11,-1))\n",
    "print(data)\n",
    "\n",
    "result = (\n",
    "    sc.parallelize(data)\n",
    "    .filter(lambda val: val % 3 == 0)\n",
    "    .map(operator.abs)\n",
    "    .fold(0, operator.add)\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60fbd933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([\"b\", \"a\", \"c\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fd240d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Lorem': 3,\n",
       "             'ipsum': 11,\n",
       "             'dolor': 10,\n",
       "             'sit': 50,\n",
       "             'amet,': 1,\n",
       "             'consectetur': 12,\n",
       "             'adipiscing': 23,\n",
       "             'elit,': 1,\n",
       "             'sed': 66,\n",
       "             'do': 1,\n",
       "             'eiusmod': 1,\n",
       "             'tempor': 11,\n",
       "             'incididunt': 1,\n",
       "             'ut': 48,\n",
       "             'labore': 1,\n",
       "             'et': 34,\n",
       "             'dolore': 1,\n",
       "             'magna': 15,\n",
       "             'aliqua.': 1,\n",
       "             'Quam': 5,\n",
       "             'lacus': 21,\n",
       "             'suspendisse': 17,\n",
       "             'faucibus': 25,\n",
       "             'interdum': 12,\n",
       "             'posuere.': 3,\n",
       "             'Dui': 5,\n",
       "             'accumsan': 10,\n",
       "             'amet': 53,\n",
       "             'nulla': 31,\n",
       "             'facilisi': 11,\n",
       "             'morbi': 25,\n",
       "             'tempus.': 2,\n",
       "             'Lobortis': 4,\n",
       "             'scelerisque': 23,\n",
       "             'fermentum': 17,\n",
       "             'dui': 21,\n",
       "             'in': 48,\n",
       "             'ornare': 16,\n",
       "             'quam': 24,\n",
       "             'viverra.': 6,\n",
       "             'In': 10,\n",
       "             'viverra': 39,\n",
       "             'orci': 21,\n",
       "             'sagittis': 19,\n",
       "             'eu.': 6,\n",
       "             'Urna': 1,\n",
       "             'duis': 10,\n",
       "             'convallis': 13,\n",
       "             'tellus': 28,\n",
       "             'id': 47,\n",
       "             'interdum.': 5,\n",
       "             'A': 5,\n",
       "             'erat': 7,\n",
       "             'nam': 5,\n",
       "             'at': 36,\n",
       "             'lectus.': 5,\n",
       "             'Eget': 9,\n",
       "             'lorem': 11,\n",
       "             'Scelerisque': 5,\n",
       "             'eu': 30,\n",
       "             'ultrices': 18,\n",
       "             'vitae': 33,\n",
       "             'auctor': 15,\n",
       "             'augue': 21,\n",
       "             'ut.': 11,\n",
       "             'Orci': 3,\n",
       "             'porta': 8,\n",
       "             'non': 26,\n",
       "             'pulvinar': 18,\n",
       "             'neque': 18,\n",
       "             'laoreet': 10,\n",
       "             'libero.': 1,\n",
       "             'Egestas': 7,\n",
       "             'rutrum': 3,\n",
       "             'pellentesque': 33,\n",
       "             'tincidunt': 22,\n",
       "             'tortor': 21,\n",
       "             'aliquam.': 8,\n",
       "             'Vivamus': 2,\n",
       "             'eget': 36,\n",
       "             'arcu.': 8,\n",
       "             'Id': 9,\n",
       "             'leo': 16,\n",
       "             'turpis.': 5,\n",
       "             'Rutrum': 2,\n",
       "             'aliquam': 24,\n",
       "             'nulla.': 2,\n",
       "             'Mauris': 3,\n",
       "             'a': 24,\n",
       "             'diam': 25,\n",
       "             'maecenas': 14,\n",
       "             'enim': 45,\n",
       "             'urna.': 5,\n",
       "             'lectus': 20,\n",
       "             'est': 16,\n",
       "             'placerat': 8,\n",
       "             'egestas.': 6,\n",
       "             'Non': 3,\n",
       "             'curabitur': 4,\n",
       "             'gravida': 19,\n",
       "             'arcu': 24,\n",
       "             'ac': 20,\n",
       "             'dignissim': 12,\n",
       "             'convallis.': 4,\n",
       "             'Proin': 3,\n",
       "             'libero': 10,\n",
       "             'nunc': 30,\n",
       "             'consequat': 11,\n",
       "             'varius': 8,\n",
       "             'sit.': 10,\n",
       "             'Sit': 12,\n",
       "             'commodo': 15,\n",
       "             'nullam': 8,\n",
       "             'vehicula': 2,\n",
       "             'a.': 5,\n",
       "             'Eleifend': 1,\n",
       "             'proin': 15,\n",
       "             'nisl': 24,\n",
       "             'rhoncus.': 2,\n",
       "             'Elementum': 5,\n",
       "             'etiam': 10,\n",
       "             'lacus.': 3,\n",
       "             'Cursus': 6,\n",
       "             'mauris': 22,\n",
       "             'Senectus': 1,\n",
       "             'netus': 10,\n",
       "             'malesuada': 17,\n",
       "             'fames': 5,\n",
       "             'turpis': 22,\n",
       "             'suspendisse.': 7,\n",
       "             'Massa': 5,\n",
       "             'egestas': 37,\n",
       "             'purus': 19,\n",
       "             'in.': 9,\n",
       "             'sodales': 4,\n",
       "             'amet.': 8,\n",
       "             'condimentum': 10,\n",
       "             'lacinia': 5,\n",
       "             'quis': 27,\n",
       "             'vel': 19,\n",
       "             'eros': 4,\n",
       "             'donec.': 2,\n",
       "             'Magna': 3,\n",
       "             'blandit.': 2,\n",
       "             'Mattis': 7,\n",
       "             'ullamcorper': 20,\n",
       "             'velit': 20,\n",
       "             'massa': 25,\n",
       "             'eget.': 10,\n",
       "             'Morbi': 7,\n",
       "             'tristique': 22,\n",
       "             'senectus': 12,\n",
       "             'Laoreet': 3,\n",
       "             'tincidunt.': 7,\n",
       "             'risus': 20,\n",
       "             'mi': 19,\n",
       "             'Fames': 1,\n",
       "             'pharetra': 14,\n",
       "             'Ultrices': 7,\n",
       "             'cursus': 20,\n",
       "             'aliquet': 19,\n",
       "             'nibh': 20,\n",
       "             'condimentum.': 3,\n",
       "             'Ipsum': 4,\n",
       "             'dictum': 13,\n",
       "             'fusce.': 1,\n",
       "             'Tempus': 2,\n",
       "             'nec': 17,\n",
       "             'nam.': 1,\n",
       "             'Parturient': 1,\n",
       "             'montes': 2,\n",
       "             'nascetur': 2,\n",
       "             'ridiculus': 1,\n",
       "             'mus': 2,\n",
       "             'ultricies.': 2,\n",
       "             'odio': 20,\n",
       "             'Amet': 8,\n",
       "             'tempus': 14,\n",
       "             'iaculis': 12,\n",
       "             'urna': 26,\n",
       "             'volutpat.': 6,\n",
       "             'Eu': 7,\n",
       "             'bibendum': 17,\n",
       "             'pharetra.': 4,\n",
       "             'Vitae': 6,\n",
       "             'rhoncus': 10,\n",
       "             'mattis': 19,\n",
       "             'elit': 21,\n",
       "             'pellentesque.': 9,\n",
       "             'hendrerit': 5,\n",
       "             'quisque': 6,\n",
       "             'auctor.': 3,\n",
       "             'Vel': 4,\n",
       "             'pulvinar.': 3,\n",
       "             'Dolor': 1,\n",
       "             'aliquet.': 4,\n",
       "             'Tortor': 2,\n",
       "             'venenatis': 10,\n",
       "             'cras': 10,\n",
       "             'felis.': 1,\n",
       "             'Nisl': 3,\n",
       "             'pretium': 21,\n",
       "             'fusce': 6,\n",
       "             'Nisi': 3,\n",
       "             'mollis': 6,\n",
       "             'Ornare': 2,\n",
       "             'nisl.': 3,\n",
       "             'Suspendisse': 1,\n",
       "             'nisi': 9,\n",
       "             'hac.': 1,\n",
       "             'dui.': 5,\n",
       "             'Imperdiet': 2,\n",
       "             'iaculis.': 6,\n",
       "             'Arcu': 3,\n",
       "             'euismod': 11,\n",
       "             'nunc.': 7,\n",
       "             'Viverra': 9,\n",
       "             'cras.': 4,\n",
       "             'Pellentesque': 6,\n",
       "             'volutpat': 22,\n",
       "             'habitant': 9,\n",
       "             'tristique.': 1,\n",
       "             'quam.': 6,\n",
       "             'Sed': 8,\n",
       "             'ultricies': 16,\n",
       "             'scelerisque.': 3,\n",
       "             'suscipit': 7,\n",
       "             'integer.': 4,\n",
       "             'fringilla': 8,\n",
       "             'porttitor': 7,\n",
       "             'enim.': 3,\n",
       "             'Enim': 10,\n",
       "             'vulputate': 21,\n",
       "             'Sapien': 3,\n",
       "             'ligula': 1,\n",
       "             'proin.': 2,\n",
       "             'Ullamcorper': 2,\n",
       "             'varius.': 2,\n",
       "             'Condimentum': 2,\n",
       "             'ac.': 2,\n",
       "             'Diam': 11,\n",
       "             'sem': 8,\n",
       "             'Ac': 7,\n",
       "             'posuere': 11,\n",
       "             'morbi.': 8,\n",
       "             'facilisis': 12,\n",
       "             'mauris.': 6,\n",
       "             'tortor.': 4,\n",
       "             'semper': 14,\n",
       "             'diam.': 4,\n",
       "             'vivamus': 6,\n",
       "             'felis': 11,\n",
       "             'Dignissim': 1,\n",
       "             'integer': 16,\n",
       "             'vitae.': 5,\n",
       "             'Integer': 5,\n",
       "             'feugiat': 12,\n",
       "             'Tellus': 7,\n",
       "             'id.': 7,\n",
       "             'Nibh': 4,\n",
       "             'odio.': 2,\n",
       "             'Adipiscing': 6,\n",
       "             'sollicitudin': 6,\n",
       "             'nibh.': 4,\n",
       "             'Cras': 2,\n",
       "             'consectetur.': 2,\n",
       "             'Aliquet': 8,\n",
       "             'luctus': 2,\n",
       "             'Consequat': 2,\n",
       "             'justo': 10,\n",
       "             'laoreet.': 2,\n",
       "             'Posuere': 1,\n",
       "             'molestie.': 3,\n",
       "             'Tempor': 3,\n",
       "             'mi.': 2,\n",
       "             'Velit': 2,\n",
       "             'Sem': 3,\n",
       "             'Congue': 2,\n",
       "             'aenean': 7,\n",
       "             'elementum': 24,\n",
       "             'sagittis.': 2,\n",
       "             'ipsum.': 5,\n",
       "             'pretium.': 4,\n",
       "             'Sollicitudin': 1,\n",
       "             'luctus.': 1,\n",
       "             'Est': 5,\n",
       "             'facilisi.': 1,\n",
       "             'Ut': 9,\n",
       "             'blandit': 11,\n",
       "             'erat.': 5,\n",
       "             'Donec': 3,\n",
       "             'sapien': 7,\n",
       "             'malesuada.': 3,\n",
       "             'bibendum.': 2,\n",
       "             'adipiscing.': 3,\n",
       "             'sed.': 13,\n",
       "             'dignissim.': 3,\n",
       "             'Leo': 2,\n",
       "             'molestie': 11,\n",
       "             'elementum.': 2,\n",
       "             'Risus': 5,\n",
       "             'lobortis': 8,\n",
       "             'faucibus.': 4,\n",
       "             'phasellus': 8,\n",
       "             'vestibulum': 13,\n",
       "             'Etiam': 2,\n",
       "             'Vestibulum': 4,\n",
       "             'ullamcorper.': 3,\n",
       "             'Sagittis': 5,\n",
       "             'Maecenas': 1,\n",
       "             'nisi.': 1,\n",
       "             'Convallis': 2,\n",
       "             'neque.': 4,\n",
       "             'porta.': 3,\n",
       "             'Aenean': 4,\n",
       "             'eleifend': 8,\n",
       "             'Porttitor': 2,\n",
       "             'risus.': 5,\n",
       "             'Elit': 5,\n",
       "             'imperdiet': 11,\n",
       "             'ante': 2,\n",
       "             'mattis.': 5,\n",
       "             'metus': 7,\n",
       "             'Bibendum': 3,\n",
       "             'At': 12,\n",
       "             'Fusce': 2,\n",
       "             'Tristique': 4,\n",
       "             'Lacus': 4,\n",
       "             'lacinia.': 1,\n",
       "             'Pretium': 7,\n",
       "             'donec': 11,\n",
       "             'Feugiat': 3,\n",
       "             'praesent': 7,\n",
       "             'feugiat.': 3,\n",
       "             'Dictum': 3,\n",
       "             'Et': 3,\n",
       "             'Malesuada': 2,\n",
       "             'accumsan.': 3,\n",
       "             'Faucibus': 4,\n",
       "             'quis.': 5,\n",
       "             'aenean.': 2,\n",
       "             'cursus.': 4,\n",
       "             'Pharetra': 4,\n",
       "             'Vulputate': 3,\n",
       "             'Augue': 2,\n",
       "             'Nec': 2,\n",
       "             'hendrerit.': 1,\n",
       "             'Libero': 1,\n",
       "             'dictum.': 3,\n",
       "             'orci.': 5,\n",
       "             'rutrum.': 2,\n",
       "             'hac': 1,\n",
       "             'habitasse': 1,\n",
       "             'platea': 1,\n",
       "             'dictumst': 1,\n",
       "             'vestibulum.': 3,\n",
       "             'Ante': 1,\n",
       "             'Fermentum': 1,\n",
       "             'ornare.': 6,\n",
       "             'Mi': 4,\n",
       "             'congue': 4,\n",
       "             'potenti.': 1,\n",
       "             'Nunc': 5,\n",
       "             'purus.': 3,\n",
       "             'facilisis.': 2,\n",
       "             'Molestie': 1,\n",
       "             'Tincidunt': 4,\n",
       "             'venenatis.': 4,\n",
       "             'Volutpat': 2,\n",
       "             'tellus.': 3,\n",
       "             'Turpis': 3,\n",
       "             'Penatibus': 1,\n",
       "             'magnis': 1,\n",
       "             'dis': 1,\n",
       "             'parturient': 1,\n",
       "             'ridiculus.': 1,\n",
       "             'Commodo': 3,\n",
       "             'non.': 4,\n",
       "             'Purus': 2,\n",
       "             'fermentum.': 4,\n",
       "             'Venenatis': 2,\n",
       "             'Lectus': 2,\n",
       "             'porttitor.': 5,\n",
       "             'vulputate.': 1,\n",
       "             'Facilisis': 1,\n",
       "             'elit.': 3,\n",
       "             'et.': 10,\n",
       "             'Auctor': 2,\n",
       "             'Euismod': 1,\n",
       "             'at.': 8,\n",
       "             'Justo': 1,\n",
       "             'phasellus.': 2,\n",
       "             'Curabitur': 1,\n",
       "             'senectus.': 3,\n",
       "             'potenti': 1,\n",
       "             'Neque': 2,\n",
       "             'placerat.': 2,\n",
       "             'cum': 2,\n",
       "             'sociis': 2,\n",
       "             'natoque.': 2,\n",
       "             'Nulla': 3,\n",
       "             'Odio': 2,\n",
       "             'Dictumst': 2,\n",
       "             'Duis': 1,\n",
       "             'Blandit': 3,\n",
       "             'lorem.': 3,\n",
       "             'imperdiet.': 2,\n",
       "             'Aliquam': 2,\n",
       "             'Ridiculus': 1,\n",
       "             'vel.': 2,\n",
       "             'Porta': 2,\n",
       "             'duis.': 2,\n",
       "             'Eros': 3,\n",
       "             'gravida.': 3,\n",
       "             'euismod.': 1,\n",
       "             'Suscipit': 1,\n",
       "             'Accumsan': 1,\n",
       "             'Pulvinar': 2,\n",
       "             'semper.': 1,\n",
       "             'dapibus': 2,\n",
       "             'ultrices.': 2,\n",
       "             'fringilla.': 2,\n",
       "             'ante.': 1,\n",
       "             'Luctus': 1,\n",
       "             'Quis': 1,\n",
       "             'Ultricies': 2,\n",
       "             'Placerat': 1,\n",
       "             'Potenti': 1,\n",
       "             'maecenas.': 2,\n",
       "             'congue.': 1,\n",
       "             'Dapibus': 1,\n",
       "             'Metus': 1,\n",
       "             'augue.': 1,\n",
       "             'est.': 2,\n",
       "             'massa.': 1,\n",
       "             'Nam': 1,\n",
       "             'Habitant': 1,\n",
       "             'netus.': 2,\n",
       "             'fames.': 1,\n",
       "             'Mollis': 1,\n",
       "             'leo.': 2,\n",
       "             'Consectetur': 1,\n",
       "             'Semper': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddDistributedFile.flatMap(lambda text: text.split()).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069261a",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "## Dataset and DataFrame\n",
    "\n",
    "Dataset is a distributed collection of data which provides:\n",
    "- strong typing and powerful lambda functions from `RDD`\n",
    "- __allows for Spark SQL optimized execution engine__\n",
    "\n",
    "It can be created from JVM objects __and manipulated in the same functional manner__.\n",
    "\n",
    "> __`pyspark` has no Dataset API but many benefits of `Dataset` are available for `DataFrame`s DUE TO IT'S DYNAMIC NATURE__\n",
    "\n",
    "DataFrame shortcomings included:\n",
    "- No compile-time safety, hence __you cannot manipulate data of which structure is not specified__\n",
    "\n",
    "> DataFrame is a a  Dataset organised into named columns (__same as for `pd.DataFrame`__)\n",
    "\n",
    "From now on we will use `DataFrame`s (__not `Dataset`, also due to Python's community similarity with `pd.DataFrame`__) to keep our records.\n",
    "\n",
    "See [this discussion](https://stackoverflow.com/questions/31508083/difference-between-dataframe-dataset-and-rdd-in-spark) for an extended description.\n",
    "\n",
    "## Creating DataFrames\n",
    "\n",
    "> __For all of the operations we can use `SparkSession` directly to interact with the cluster!__\n",
    "\n",
    "There are a few options usable for us to read data residing on clusters (__for each node it has to be at the same location if reading from file!__):\n",
    "- [`session.createDataFrame`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html#pyspark.sql.SparkSession.createDataFrame) - create `pyspark.sql.DataFrame` from:\n",
    "    - `RDD`\n",
    "    - `list`\n",
    "    - `pandas.DataFrame`\n",
    "    - __Optionally: with `schema`__ which specifies datatypes and format for data contained within it. See documentation for more info.\n",
    "    - By default `schema` is inferred if possible\n",
    "- [`session.range`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.range.html#pyspark.sql.SparkSession.range) - works like Python's range but distributed and as a `spark.DataFrame`\n",
    "- [`session.sql(query)`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql) - __return DataFrame which represents result of `sql` query__\n",
    "- [`session.read.{how_to_read}()`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) - __returns `DataFrameReader` object__ which allows us to read `df` from:\n",
    "    - `json`\n",
    "    - `parquet`\n",
    "    - `csv`\n",
    "    - and many more\n",
    "- [`session.readStream`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.SparkSession.readStream.html#pyspark.sql.SparkSession.readStream) - __used for streaming, we will see it a little later__\n",
    "\n",
    "Let's see some code with `pyspark.sql.DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d903104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "| 10|  1| 63| 97|\n",
      "| 15| 84| 33| 85|\n",
      "| 50| 33| 73|  9|\n",
      "| 49|  3| 12| 52|\n",
      "| 49|  4| 35| 51|\n",
      "| 31| 28| 76| 42|\n",
      "| 99|  5| 49| 19|\n",
      "| 71| 66| 68| 50|\n",
      "| 45| 48|  7| 32|\n",
      "| 92| 37| 84| 19|\n",
      "| 40| 53| 63| 30|\n",
      "| 43| 22| 64| 46|\n",
      "| 66| 49|  6| 67|\n",
      "| 20| 41| 21| 71|\n",
      "| 40| 93| 81|  9|\n",
      "| 50| 78|  6| 31|\n",
      "| 64| 62| 18| 43|\n",
      "| 52| 33| 37| 50|\n",
      "| 75| 88| 65| 82|\n",
      "| 90|  8| 87|  5|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = session.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        np.random.randint(0, 100, size=(100, 4)),\n",
    "        columns=list(\"ABCD\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e38c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: long (nullable = true)\n",
      " |-- B: long (nullable = true)\n",
      " |-- C: long (nullable = true)\n",
      " |-- D: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a1468c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  A|\n",
      "+---+\n",
      "| 10|\n",
      "| 15|\n",
      "| 50|\n",
      "| 49|\n",
      "| 49|\n",
      "| 31|\n",
      "| 99|\n",
      "| 71|\n",
      "| 45|\n",
      "| 92|\n",
      "| 40|\n",
      "| 43|\n",
      "| 66|\n",
      "| 20|\n",
      "| 40|\n",
      "| 50|\n",
      "| 64|\n",
      "| 52|\n",
      "| 75|\n",
      "| 90|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show is an action, nothing would be returned without it\n",
    "# Just an operation representing what will happen\n",
    "df.select(\"A\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e40cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[A: bigint, (B + 1): bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df[\"A\"], df[\"B\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "785ba5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|  A|(B + 1)|\n",
      "+---+-------+\n",
      "| 10|      2|\n",
      "| 15|     85|\n",
      "| 50|     34|\n",
      "| 49|      4|\n",
      "| 49|      5|\n",
      "| 31|     29|\n",
      "| 99|      6|\n",
      "| 71|     67|\n",
      "| 45|     49|\n",
      "| 92|     38|\n",
      "| 40|     54|\n",
      "| 43|     23|\n",
      "| 66|     50|\n",
      "| 20|     42|\n",
      "| 40|     94|\n",
      "| 50|     79|\n",
      "| 64|     63|\n",
      "| 52|     34|\n",
      "| 75|     89|\n",
      "| 90|      9|\n",
      "+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Increase column value by one\n",
    "# This operation is shown in the output\n",
    "\n",
    "df.select(df[\"A\"], df[\"B\"] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33de5702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  B|count|\n",
      "+---+-----+\n",
      "| 26|    3|\n",
      "| 84|    2|\n",
      "| 98|    2|\n",
      "| 71|    2|\n",
      "|  6|    2|\n",
      "| 27|    2|\n",
      "| 51|    3|\n",
      "| 41|    2|\n",
      "| 33|    2|\n",
      "| 28|    3|\n",
      "| 88|    3|\n",
      "| 48|    5|\n",
      "| 44|    2|\n",
      "|  3|    3|\n",
      "| 37|    2|\n",
      "| 62|    3|\n",
      "| 59|    2|\n",
      "| 15|    2|\n",
      "| 38|    2|\n",
      "| 46|    4|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counted = df.groupby(\"B\").count().persist()\n",
    "counted.filter(counted[\"count\"] > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81011b59",
   "metadata": {},
   "source": [
    "## Operations on DataFrame\n",
    "\n",
    "> __`pyspark.sql.DataFrame` supports most of the `pd.DataFrame` operations + the RDD ones__\n",
    "\n",
    "You can see the whole list [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)\n",
    "\n",
    "> __In general one can work with it similarly to how one works with `pd.DataFrame` objects__\n",
    "\n",
    "there are a few exceptions though...\n",
    "\n",
    "## Running SQL queries\n",
    "\n",
    "> In order to run SQL queries against the DataFrame __we have to register them as `TemporaryViews`__\n",
    "\n",
    "Properties of `TemporaryViews`:\n",
    "- __Session scoped__ - if session runs out of scope so will the views registered for it\n",
    "- One can set up `DataFrame` globally for any `SparkSession` by using `df.createGlobalTempView(\"name_of_database\")`\n",
    "\n",
    "After that, we can run SQL queries against __distributed data across nodes__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "420709b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "| 10|  1| 63| 97|\n",
      "| 15| 84| 33| 85|\n",
      "| 50| 33| 73|  9|\n",
      "| 49|  3| 12| 52|\n",
      "| 49|  4| 35| 51|\n",
      "| 31| 28| 76| 42|\n",
      "| 99|  5| 49| 19|\n",
      "| 71| 66| 68| 50|\n",
      "| 45| 48|  7| 32|\n",
      "| 92| 37| 84| 19|\n",
      "| 40| 53| 63| 30|\n",
      "| 43| 22| 64| 46|\n",
      "| 66| 49|  6| 67|\n",
      "| 20| 41| 21| 71|\n",
      "| 40| 93| 81|  9|\n",
      "| 50| 78|  6| 31|\n",
      "| 64| 62| 18| 43|\n",
      "| 52| 33| 37| 50|\n",
      "| 75| 88| 65| 82|\n",
      "| 90|  8| 87|  5|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"any_name\")\n",
    "\n",
    "# WE USE SESSION TO RUN QUERIES!\n",
    "sqlDf = session.sql(\"SELECT * FROM any_name\")\n",
    "sqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd8677",
   "metadata": {},
   "source": [
    "# Spark-Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673c112",
   "metadata": {},
   "source": [
    "The work you see in this notebook sent applications to a clusted interactively, meaning that you were running all cells sequentially. \n",
    "\n",
    "In a production environment, you are more likely to launch the applications from a script, where that script contains all the operations using PySpark. \n",
    "\n",
    "To do so, you can use spark-submit, which can be ran from the terminal to _submit_ your Spark applications. The syntax is as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c275d",
   "metadata": {},
   "source": [
    "```\n",
    "spark-submit \\\n",
    "  --class <main-class> \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <deploy-mode> \\\n",
    "  --conf <key>=<value> \\\n",
    "  ... # other options\n",
    "  <application-jar> \\\n",
    "  [application-arguments]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac83e21",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "- __class__ is the entrypoint for your application\n",
    "- __master__ the URL of your cluster. You can set it to `local` to run it locally\n",
    "- __deploy-mode__ Whether to deploy on the worker or locally as a client\n",
    "- __conf__ Configuration of the Spark application in a `key=value` way\n",
    "- __application-jar__: Path to a your application\n",
    "\n",
    "Within other options, you can specify number of workers or the number of cores:\n",
    "\n",
    "- __--num-executors__\n",
    "- __--num-cores__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73cc55",
   "metadata": {},
   "source": [
    "In this case, we are going to submit the same example we were working with. This application will print put the words in lorem, and the number of occurences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.py\n",
    "\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    " \n",
    "    # create Spark context with Spark configuration\n",
    "    conf = SparkConf().setAppName(\"Word Count - Python\").setMaster('local[*]')\n",
    "    session = pyspark.sql.SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "    # read in text file and split each document into words\n",
    "    rddDistributedFile = session.sparkContext.textFile(\"lorem.txt\")\n",
    "    rddDistributedFile = rddDistributedFile.cache()\n",
    "    # count the occurrence of each word\n",
    "    print(rddDistributedFile.flatMap(lambda text: text.split()).countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d8610",
   "metadata": {},
   "source": [
    "In this repo, you will find a `example.py` files that you can try for submitting your application. You can run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5af65",
   "metadata": {},
   "source": [
    "`<SPARK_HOME>/bin/spark-submit.cmd example.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3decf223",
   "metadata": {},
   "source": [
    "If you encounter an error, you might need to paste a file `winutils.exe` for running the command above. You can download the corresponding version [here](https://github.com/steveloughran/winutils).\n",
    "\n",
    "Your directory should look like this:\n",
    "\n",
    "```\n",
    "~/\n",
    "│\n",
    "├── spark/\n",
    "│   └── spark-3.1.2-bin-hadoop3.2  <--- SPARK_HOME\n",
    "│         ├── bin\n",
    "│         ├── conf\n",
    "│         ├── data\n",
    "│         ├── examples\n",
    "│         ├── hadoop               <--- Add this new folder = HADOOP_HOME\n",
    "│         │    └── bin\n",
    "│         │         └── winutils.exe\n",
    "... \n",
    "```\n",
    "\n",
    "Then, you'll have to set the a new environment variable `HADOOP_HOME` with the directory of the folder `hadoop`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ca66b",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Check out [`rdd.aggregate`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.aggregate.html#pyspark.RDD.aggregate) method for RDDs.\n",
    "- What is the difference between `forEach` and `map`? Check [this StackOverflow answer](https://stackoverflow.com/questions/354909/is-there-a-difference-between-foreach-and-map) if in doubt\n",
    "- What is the difference between `reduce` and `fold`? check [this StackOverflow answer](https://stackoverflow.com/a/36060141/10886420). Which one is \"safer\" to use?\n",
    "- Which operations on RDDs induce `shuffle` and why is it a problem? See [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations) for more info\n",
    "- Check how to use [Hive](https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html) with PySpark. What Hive is and how it differs from SQL?\n",
    "- Check out how to specify schema programmaticaly (presented [in this tutorial](https://spark.apache.org/docs/latest/sql-getting-started.html#programmatically-specifying-the-schema)). What are the upsides/downsides of using it?\n",
    "\n",
    "- Read more about multiple `SparkContext`s and `SparkSession`s and why would we need it in some... contexts. Check it [over here](https://www.waitingforcode.com/apache-spark-sql/multiple-sparksession-one-sparkcontext/read)\n",
    "- What is [`rdd.meanApprox`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.meanApprox.html#pyspark.RDD.meanApprox) and why might we need it?\n",
    "- Generally discouraged, but what are the options to share data between tasks and nodes in the cluster? Check out [this part of RDD tutorial](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shared-variables)\n",
    "- Check [performance tuning options for `spark.sql`](https://spark.apache.org/docs/latest/sql-performance-tuning.html). One can use them when creating `pyspark.SparkConf()` object"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6971ff02672853a145ab8a619e17e4c2b989e1ba4684228133b86b474ce57f92"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
