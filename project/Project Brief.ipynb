{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Pipelines project: Your first data pipeline\n",
    "\n",
    "## What?\n",
    "The main idea is that you'll use web scraping to collect data from the internet.\n",
    "You'll put this data together to create a dataset which you can use for future projects.\n",
    "\n",
    "In your next class, you will be introduced to machine learning. You'll learn that the data is an enabling component which big machine learning systems depend on. You'll also learn that supervised learning is just finding a mathematical function which maps from input to output, and that the dataset defines these inputs and outputs. For this project, the dataset you collect should be supervised.\n",
    "\n",
    "You should also use cron to schedule your Python web scraping script so that it runs automatically at intervals.\n",
    "\n",
    "## Why?\n",
    "Creating a dataset relevant to work that you want to do in the future will give you something uniquely interesting to companies that you might apply to.\n",
    "\n",
    "By creating your own dataset, you can start working on an interesting problem that might be relevant to awesome people that you can reach out to. Later in the course, we'll help introduce you to people who can give advice on the kind of problems you're working on.\n",
    "\n",
    "What problems do you think you might you be able to tackle using AI? What interests you? What kind of data would jobs you want to apply to value your experience in working with?\n",
    "\n",
    "## Deliverables\n",
    "- A Github repo containing all of the code\n",
    "- Obviously, the dataset (probably not pushed to GitHub because it will be huge)\n",
    "    - it must contain at least one numerical feature and one numerical or categorical label (so we can apply ML in the next unit)\n",
    "    - at least 1000 examples\n",
    "- A slideshow presentation explaining\n",
    "    - the different locations on the internet that your script collects data from\n",
    "    - the layout of a example webpages you scraped and how you targeted elements within them\n",
    "    - in what format you chose to store your data and why (data lake vs data warehouse, file type, database table schema)\n",
    "    - how you cleaned the data\n",
    "    - suggestions of which variable in your dataset you will attempt to predict using some of the remaining variables\n",
    "    - do not include screenshots of code\n",
    "- a contribution to the datasets module of the ai_core Python libary [added late]\n",
    "\n",
    "## Deadline\n",
    "The project deadline is for 2 weeks from when announced.\n",
    "\n",
    "## Marking criteria\n",
    "\n",
    "Each of the bullets under the following headings will be scored as \"not attempted\" (0 points), \"attempted\" (1 point) or \"successfully applied\" (2 points).\n",
    "\n",
    "## Readability\n",
    "- Your repository has a readme file summarising what the project does, the motivation behind it, how to use it and what you achieved\n",
    "- You added comments explaining each decision and docstrings for each function/class\n",
    "- Your repo contains all the elements necessary to run your code and does not contain depricated files\n",
    "\n",
    "### Programming\n",
    "- code is object oriented with logical chunks of code within functions, and related functions encapsulated withing classes\n",
    "- `.py` files\n",
    "\n",
    "### Data storage\n",
    "- Tabular data stored in a SQL database\n",
    "- Raw data stored in a data lake (S3), if appropriate\n",
    "- Images, if part of the dataset, are downloaded, stored in a data lake and named by example id, as in the tabular data.\n",
    "\n",
    "### Data cleaning\n",
    "- Your structured data contains a single variable per column\n",
    "- You handled duplicates and nulls properly\n",
    "\n",
    "## Presentation\n",
    "- presentation is part of the repository\n",
    "- presentation lasted between 5 and 11 minutes (please, rehearse)\n",
    "\n",
    "## Gold stars\n",
    "- Special prize for the individual who collects the largest dataset.\n",
    "- Can you figure out how to run the scraper on a remote server using AWS (or another cloud provider) so that it doesn't run on your local machine? Don't worry, all the cloud providers give free credits.\n",
    "- Your data contains way more than 100K rows\n",
    "- Your data contains tabular data, free text & images\n",
    "- You used 2 or more sources of data in your project\n",
    "- used multithreading to accelerate the scraping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}